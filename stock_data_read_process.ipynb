{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import os\n",
    "from alpha_vantage.timeseries import TimeSeries          # stock time series data API\n",
    "from alpha_vantage.techindicators import TechIndicators        # Technical Indicator data API\n",
    "import dateutil.parser\n",
    "from Metric_Computation import *\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dfine the function that query one stocl data, return the value and store the corresponding csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_data_read_and_process(stock_symbol, query_key, stock_save_path, start_period, end_period, S_P_500_date_array, S_P_500_value_array, series_length=50, fluc_check_period=20, silent_mode=True):\n",
    "    '''\n",
    "    :param stock_symbol: the symbol of the stock we are interested\n",
    "    :param query_key: the key for us to use the Alpha Vantage API\n",
    "    :param stock_save_path: the path to save the files\n",
    "    :param start_period: the starting period\n",
    "    :param end_period: the ending period\n",
    "    :param S_P_500_date_array: the array passing S&P-500 dates (for storage)\n",
    "    :param S_P_500_value_array: the array passing S&P-500 values\n",
    "    :param series_length: the length of series we would like to query. Default is 50. Must be at least >(end_period-start_period+10)\n",
    "    :param fluc_check_period: the period to check up/down trends and if it matches in/de-crease quantities. default is 20\n",
    "    :param silent_mode: If the function will be printing information of the stock reading procedure\n",
    "    return: X_data, containing all the time series data\n",
    "            y_gradient: the target of gradient (difference)\n",
    "            y_price: the target of real-price (only for reference usage)\n",
    "            (the information of each stock will be stored in a .csv file separately)\n",
    "    '''\n",
    "    # **********************Price Time Series Query*****************\n",
    "    if silent_mode is False:\n",
    "        print('Query the stock...')\n",
    "    query_gate_ts = TimeSeries(key=alpha_van_key_1,\n",
    "                               retries=5,\n",
    "                               output_format='pandas')\n",
    "    exp_daily_adj_series, _ = query_gate_ts.get_daily_adjusted(symbol=stock_symbol,\n",
    "                                                               outputsize='full')\n",
    "    exp_daily_adj_series.index = pd.to_datetime(exp_daily_adj_series.index)\n",
    "    if silent_mode is False:\n",
    "        print('Stock Series Query Completed!')\n",
    "    # ********************retrieve the high, low, and adjusted closed prices******************\n",
    "    # exp_daily_adj_series = exp_daily_adj_series[df['column_name'] == ]\n",
    "    # only care the data after 2018 -- we are not responsible for major economic bust!\n",
    "    if exp_daily_adj_series.index.tolist()[0]<datetime.strptime('2009-01-01','%Y-%m-%d'):\n",
    "        exp_daily_adj_series = exp_daily_adj_series[exp_daily_adj_series.index>=datetime.strptime('2009-01-01','%Y-%m-%d')]\n",
    "    # collect dates\n",
    "    date_list_series = np.array(exp_daily_adj_series.index.tolist(),dtype='object')\n",
    "    # collect the high, low and adjusted close prices\n",
    "    high_price_vec = np.array(exp_daily_adj_series.loc[:,'2. high'].values.tolist())\n",
    "    low_price_vec = np.array(exp_daily_adj_series.loc[:,'3. low'].values.tolist())\n",
    "    close_price_vec = np.array(exp_daily_adj_series.loc[:,'5. adjusted close'].values.tolist())\n",
    "    #*************************** make time series **************************\n",
    "    if silent_mode is False:\n",
    "        print('Constructing the time series...')\n",
    "    # compute the nData\n",
    "    # \"-1\" for one day difference (essential for fluctuation-control)\n",
    "    nData = close_price_vec.shape[0] - series_length - fluc_check_period - 1\n",
    "    # placeholders of a empty time series array\n",
    "    # current and 1-time-unit-lag adjusted close\n",
    "    time_series_array = np.zeros([nData,series_length])\n",
    "    time_series_array_prev = np.zeros([nData,series_length])\n",
    "    # current and 1-time-unit-lag high\n",
    "    high_series_array = np.zeros([nData,series_length])\n",
    "    high_series_array_prev = np.zeros([nData,series_length])\n",
    "    # current and 1-time-unit-lag low\n",
    "    low_series_array = np.zeros([nData,series_length])\n",
    "    low_series_array_prev = np.zeros([nData,series_length])\n",
    "    # placeholder of target gradient and target value\n",
    "    target_value_array = np.zeros([nData])\n",
    "    target_value_array_prev = np.zeros([nData])\n",
    "    target_gradient_array = np.zeros([nData])\n",
    "    target_gradient_array_prev = np.zeros([nData])\n",
    "    # placeholder of target flags -- no need for 'previous' lag array because they are only for regression\n",
    "    inc_dec_flag_array = np.zeros([nData])\n",
    "    up_down_trend_flag_array = np.zeros([nData])\n",
    "    # plcaholder of the corresponding SP-500 values\n",
    "    S_P_500_stock = np.zeros([nData])\n",
    "    S_P_500_stock_prev = np.zeros([nData])\n",
    "    for c_series_ind in range(nData+1):\n",
    "        current_backward_ind = c_series_ind + fluc_check_period\n",
    "        if c_series_ind<=(nData-1):\n",
    "            # assign current sub-vectors\n",
    "            time_series_array[c_series_ind,:] = close_price_vec[-(current_backward_ind+series_length):-(current_backward_ind)]\n",
    "            high_series_array[c_series_ind, :] = high_price_vec[-(current_backward_ind+series_length):-(current_backward_ind)]\n",
    "            low_series_array[c_series_ind, :] = low_price_vec[-(current_backward_ind+series_length):-(current_backward_ind)]\n",
    "        # assign 'previous' values\n",
    "        time_series_array_prev[c_series_ind-1,:] = close_price_vec[-(current_backward_ind+series_length):-(current_backward_ind)]\n",
    "        high_series_array_prev[c_series_ind-1, :] = high_price_vec[-(current_backward_ind+series_length):-(current_backward_ind)]\n",
    "        low_series_array_prev[c_series_ind-1, :] = low_price_vec[-(current_backward_ind+series_length):-(current_backward_ind)]\n",
    "        # compute current value and gradient\n",
    "        current_target_value = close_price_vec[-(current_backward_ind)]\n",
    "        current_target_gradient = close_price_vec[-(current_backward_ind)] - close_price_vec[-(current_backward_ind+1)]\n",
    "        # assign the value and gradient to the corresponding place\n",
    "        if c_series_ind<=(nData-1):\n",
    "            target_value_array[c_series_ind] = current_target_value\n",
    "            target_gradient_array[c_series_ind] = current_target_gradient\n",
    "        # assign the value and gradient to the corresponding 'previous' places\n",
    "        target_value_array_prev[c_series_ind-1] = current_target_value\n",
    "        target_gradient_array_prev[c_series_ind-1] = current_target_gradient\n",
    "        # retrieve the 'future price array' to compute the symbols\n",
    "        if c_series_ind==0:\n",
    "            current_future_series_array = close_price_vec[-(current_backward_ind):]\n",
    "        else:\n",
    "            current_future_series_array = close_price_vec[-(current_backward_ind):-(c_series_ind)]\n",
    "        # retrieve the last price from the 'past and now' price array\n",
    "        current_base_price = close_price_vec[-(current_backward_ind+1)]   # the last 'current' adjusted close price\n",
    "        # compute and assign the increase/decrease and uptrend/downtrend symbols\n",
    "        if c_series_ind<=(nData-1):\n",
    "            # increase/decrease 5% flag\n",
    "            inc_dec_flag_array[c_series_ind] = increase_decrease_flag_check(price_series = current_future_series_array,\n",
    "                                                                            base_price = current_base_price)\n",
    "            # the trend check flag\n",
    "            up_down_trend_flag_array[c_series_ind] = MA_up_down_check(price_series = current_future_series_array)\n",
    "        # check current S&P 500\n",
    "        current_date = date_list_series[-(c_series_ind+1)].replace(day=1)\n",
    "        current_S_P_ind = np.where(S_P_500_date_array==current_date)[0]\n",
    "        current_S_P_value = S_P_500_value_array[current_S_P_ind]\n",
    "        if c_series_ind<=(nData-1):\n",
    "            S_P_500_stock[c_series_ind] = current_S_P_value\n",
    "        S_P_500_stock_prev[c_series_ind-1] = current_S_P_value\n",
    "    if silent_mode is False:\n",
    "        # print to check the time series array\n",
    "        print('Time series construction completed!')\n",
    "    # ********************Compute the metrics*********************\n",
    "    compute_period_list = list(range(start_period,end_period,1))\n",
    "    # *SMA\n",
    "    if silent_mode is False:\n",
    "        print('Computing simple moving average...')\n",
    "    SMA_series_array = SMA_batch_compute(time_series_array,compute_period_list,comp_mode='vector')\n",
    "    SMA_series_array_prev = SMA_batch_compute(time_series_array_prev,compute_period_list,comp_mode='vector')\n",
    "    if silent_mode is False:\n",
    "        print('Simple moving average computation completed!')\n",
    "    # *EMA\n",
    "    if silent_mode is False:\n",
    "        print('Computing exponential moving average...')\n",
    "    EMA_series_array = EMA_batch_computation(time_series_array,compute_period_list,comp_mode='vector')\n",
    "    EMA_series_array_prev = EMA_batch_computation(time_series_array_prev,compute_period_list,comp_mode='vector')\n",
    "    if silent_mode is False:\n",
    "        print('Exponential moving average computation completed!')\n",
    "    # *Stochastic Oscillator\n",
    "    if silent_mode is False:\n",
    "        print('Computing Stochastic Oscillator...')\n",
    "    stoch_K_series_array, stoch_D_series_array = STOCH_batch_compute(time_series_array,\n",
    "                                                                     high_series_array,\n",
    "                                                                     low_series_array,\n",
    "                                                                     compute_period_list)\n",
    "    stoch_K_series_array_prev, stoch_D_series_array_prev = STOCH_batch_compute(time_series_array_prev,\n",
    "                                                                               high_series_array_prev,\n",
    "                                                                               low_series_array_prev,\n",
    "                                                                               compute_period_list)\n",
    "    if np.isnan(np.sum(stoch_K_series_array)):\n",
    "        stoch_K_series_array = Nan_batch_interpolation(stoch_K_series_array)\n",
    "    if np.isnan(np.sum(stoch_K_series_array_prev)):\n",
    "        stoch_K_series_array_prev = Nan_batch_interpolation(stoch_K_series_array_prev)\n",
    "    if np.isnan(np.sum(stoch_D_series_array)):\n",
    "        stoch_D_series_array = Nan_batch_interpolation(stoch_D_series_array)\n",
    "    if np.isnan(np.sum(stoch_D_series_array_prev)):\n",
    "        stoch_D_series_array_prev = Nan_batch_interpolation(stoch_D_series_array_prev)\n",
    "    if silent_mode is False:\n",
    "        print('Stochastic Oscillator Completed!')\n",
    "    # *ADX metrics\n",
    "    if silent_mode is False:\n",
    "        print('Computing ADX metrics...')\n",
    "    DI_up_series_array, DI_down_series_array = ADX_batch_compute(time_series_array,\n",
    "                                                                 high_series_array,\n",
    "                                                                 low_series_array,\n",
    "                                                                 compute_period_list)\n",
    "    DI_up_series_array_prev, DI_down_series_array_prev = ADX_batch_compute(time_series_array_prev,\n",
    "                                                                           high_series_array_prev,\n",
    "                                                                           low_series_array_prev,\n",
    "                                                                           compute_period_list)\n",
    "    if np.isnan(np.sum(DI_up_series_array)):\n",
    "        DI_up_series_array = Nan_batch_interpolation(DI_up_series_array)\n",
    "    if np.isnan(np.sum(DI_down_series_array)):\n",
    "        DI_down_series_array = Nan_batch_interpolation(DI_down_series_array)\n",
    "    if np.isnan(np.sum(DI_up_series_array_prev)):\n",
    "        DI_up_series_array_prev = Nan_batch_interpolation(DI_up_series_array_prev)\n",
    "    if np.isnan(np.sum(DI_down_series_array_prev)):\n",
    "        DI_down_series_array_prev = Nan_batch_interpolation(DI_down_series_array_prev)\n",
    "    if silent_mode is False:\n",
    "        print('ADX metrics computation completed!')\n",
    "    # *CCI\n",
    "    if silent_mode is False:\n",
    "        print('Computing CCI index...')\n",
    "    CCI_series_array = CCI_batch_compute(time_series_array,\n",
    "                                         high_series_array,\n",
    "                                         low_series_array,\n",
    "                                         compute_period_list)\n",
    "    CCI_series_array_prev = CCI_batch_compute(time_series_array_prev,\n",
    "                                              high_series_array_prev,\n",
    "                                              low_series_array_prev,\n",
    "                                              compute_period_list)\n",
    "    if np.isnan(np.sum(CCI_series_array)):\n",
    "        CCI_series_array = Nan_batch_interpolation(CCI_series_array)\n",
    "    if np.isnan(np.sum(CCI_series_array_prev)):\n",
    "        CCI_series_array_prev = Nan_batch_interpolation(CCI_series_array_prev)\n",
    "    if silent_mode is False:\n",
    "        print('CCI index computation completed!')\n",
    "    # ******************* Numpy Array for Data **************************\n",
    "    # numpy array should be [nData * nTime * nDim]\n",
    "    # non-for-loop idea concatenate the numpy arrays with a additional dimension, and flip the 2nd dim to get 'old-to-new' result\n",
    "    stock_data_X = np.concatenate([SMA_series_array[:,:,np.newaxis],\n",
    "                                   EMA_series_array[:,:,np.newaxis],\n",
    "                                   stoch_K_series_array[:,:,np.newaxis],\n",
    "                                   stoch_D_series_array[:,:,np.newaxis],\n",
    "                                   DI_up_series_array[:,:,np.newaxis],\n",
    "                                   DI_down_series_array[:,:,np.newaxis],\n",
    "                                   CCI_series_array[:,:,np.newaxis]],axis=-1)\n",
    "    # flip the time axis to arrange the array into 'old-to-new' order\n",
    "    stock_data_X = np.flip(stock_data_X,axis=1)\n",
    "    # also for the 'previous data'\n",
    "    stock_data_X_prev = np.concatenate([SMA_series_array_prev[:,:,np.newaxis],\n",
    "                                        EMA_series_array_prev[:,:,np.newaxis],\n",
    "                                        stoch_K_series_array_prev[:,:,np.newaxis],\n",
    "                                        stoch_D_series_array_prev[:,:,np.newaxis],\n",
    "                                        DI_up_series_array_prev[:,:,np.newaxis],\n",
    "                                        DI_down_series_array_prev[:,:,np.newaxis],\n",
    "                                        CCI_series_array_prev[:,:,np.newaxis]],axis=-1)\n",
    "    # flip the time axis to arrange the array into 'old-to-new' order\n",
    "    stock_data_X_prev = np.flip(stock_data_X_prev,axis=1)\n",
    "    # ******************** pandas dataframe and save as .csv ***********************\n",
    "    # specify the list of column names\n",
    "    col_names = ['SMA_period_'+str(date) for date in range(start_period,end_period)] + ['EMA_period_'+str(date) for date in range(start_period,end_period)] + ['Stoch_K_period_'+str(date) for date in range(start_period,end_period)]+ ['Stoch_D_period_'+str(date) for date in range(start_period,end_period)]+ ['DI_up_'+str(date) for date in range(start_period,end_period)]+ ['DI_down_'+str(date) for date in range(start_period,end_period)]+ ['CCI_'+str(date) for date in range(start_period,end_period)] + ['S&P_500','Price-change Flag','Trend Flag','Target Value','Target Gradient']\n",
    "    # spcify the data to concatenate\n",
    "    stock_data_pd = np.concatenate([SMA_series_array[:],\n",
    "                                    EMA_series_array[:],\n",
    "                                    stoch_K_series_array[:],\n",
    "                                    stoch_D_series_array[:],\n",
    "                                    DI_up_series_array[:],\n",
    "                                    DI_down_series_array[:],\n",
    "                                    CCI_series_array[:],\n",
    "                                    np.reshape(S_P_500_stock,[nData,1]),\n",
    "                                    np.reshape(inc_dec_flag_array,[nData,1]),\n",
    "                                    np.reshape(up_down_trend_flag_array,[nData,1]),\n",
    "                                    np.reshape(target_value_array,[nData,1]),\n",
    "                                    np.reshape(target_gradient_array,[nData,1])],axis=1)\n",
    "    # specify the index\n",
    "    stock_time_index_pd = np.flip(date_list_series[-nData:],axis=0)\n",
    "    # create the pandas dataframe object\n",
    "    this_stock_info_processed_pd = pd.DataFrame(data=stock_data_pd, index=stock_time_index_pd, columns=col_names)\n",
    "    # save pandas as .csv file\n",
    "    pandas_save_path = stock_save_path+'stock_info_csv/'\n",
    "    if not os.path.exists(pandas_save_path):\n",
    "        os.makedirs(pandas_save_path)\n",
    "    # specify file name\n",
    "    file_name = stock_symbol+'_price_info.csv'\n",
    "    # write\n",
    "    this_stock_info_processed_pd.to_csv(pandas_save_path+file_name)\n",
    "    \n",
    "    return stock_data_X, stock_data_X_prev, S_P_500_stock, S_P_500_stock_prev,  target_value_array, target_value_array_prev, target_gradient_array, target_gradient_array_prev,  inc_dec_flag_array, up_down_trend_flag_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the path of the stock list\n",
    "stock_symbol_list_path = '../data/stock_list/'\n",
    "# define the path to read sp500 index\n",
    "s_p_500_file_path = '../data/s-and-p-500/'\n",
    "# define the path to save the stock information\n",
    "stock_info_save_path = '../data/intermediate/'\n",
    "# create the path if it isn't already existed\n",
    "if not os.path.exists(stock_info_save_path):\n",
    "    os.makedirs(stock_info_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process the S&P 500 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pre-2017 data\n",
    "s_p_pre_2017_df = pd.read_csv(s_p_500_file_path+'data_csv.csv')\n",
    "# post-2017 data\n",
    "s_p_post_2017_df = pd.read_csv(s_p_500_file_path+'data_daily_17_18.csv')\n",
    "# convert the dates in the pre_2017 data to 'date' format\n",
    "pre_2017_date_series = pd.to_datetime(s_p_pre_2017_df.loc[:,'Date'])\n",
    "s_p_pre_2017_df.loc[:,'Date'] = pre_2017_date_series\n",
    "# similarly, convert the dates in the post_2017 data to 'date' format\n",
    "post_2017_date_series = pd.to_datetime(s_p_post_2017_df.loc[:,'Date'])\n",
    "s_p_post_2017_df.loc[:,'Date'] = post_2017_date_series\n",
    "# construct a mothly array and check\n",
    "nMonth = 119          # 2009-01 to 2018-11\n",
    "current_date = datetime.strptime('2009-01','%Y-%m')\n",
    "threshold_date = datetime.strptime('2018-01','%Y-%m')    # need to change to the other file\n",
    "# initialize two empty lists to store the values\n",
    "S_P_500_date_list = []\n",
    "S_P_500_value_list = []\n",
    "for cMonth in range(nMonth):\n",
    "    # compute the end date\n",
    "    end_date = current_date+relativedelta(months=+1)\n",
    "    if current_date<threshold_date:\n",
    "        current_df_sp_500 = s_p_pre_2017_df[(s_p_pre_2017_df['Date']>=current_date)&(s_p_pre_2017_df['Date']<end_date)]\n",
    "        current_value_sp_500 = np.mean(current_df_sp_500.loc[:,'SP500'].values)\n",
    "    else:\n",
    "        current_df_sp_500 = s_p_post_2017_df[(s_p_post_2017_df['Date']>=current_date)&(s_p_post_2017_df['Date']<end_date)]\n",
    "        current_value_sp_500 = np.mean(current_df_sp_500.loc[:,'Adj Close'].values)\n",
    "    # assign the values\n",
    "    S_P_500_date_list.append(current_date)\n",
    "    S_P_500_value_list.append(current_value_sp_500)\n",
    "    # replace the current date for date growth\n",
    "    current_date = end_date\n",
    "S_P_500_date_array = np.array(S_P_500_date_list)\n",
    "S_P_500_value_array = np.array(S_P_500_value_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the stock symbols and specify hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the stock symbol file\n",
    "Nasdaq_100_df = pd.read_csv(stock_symbol_list_path+'nasdaq100list.csv')\n",
    "# print(Nasdaq_100_df)\n",
    "stock_symbol_list = Nasdaq_100_df.loc[:,'Symbol'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the period list\n",
    "start_period = 1     # which means start_period-1, from some day\n",
    "end_period = 25   # which means up to (compute_period_list-1)\n",
    "alpha_van_key_1 = 'CDG9CCIB6BNCIU6A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursively call the function to load and process the stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading and processing stock data, 0.0% has been finished..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vcmo/Desktop/RutgersCourseMaterial/PrincipleOfAI/project/code/Metric_Computation.py:335: RuntimeWarning: invalid value encountered in true_divide\n",
      "  CCI_rst_mat = (1/0.015)*np.divide(typical_price_mat-MA_tp_mat, MA_MD_mat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and processing stock data, 12.62135922330097% has been finished...."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vcmo/Desktop/RutgersCourseMaterial/PrincipleOfAI/project/code/Metric_Computation.py:268: RuntimeWarning: invalid value encountered in true_divide\n",
      "  DI_up_mat = 100*np.divide(DM_up_SMA,TR_mat)\n",
      "/Users/vcmo/Desktop/RutgersCourseMaterial/PrincipleOfAI/project/code/Metric_Computation.py:269: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  DI_down_mat = 100*np.divide(DM_down_SMA,TR_mat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and processing stock data, 22.33009708737864% has been finished...."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vcmo/Desktop/RutgersCourseMaterial/PrincipleOfAI/project/code/Metric_Computation.py:185: RuntimeWarning: invalid value encountered in true_divide\n",
      "  period_K_metric = 100*(close_price-period_lowest_low)/(period_highest_high-period_lowest_low)\n",
      "/Users/vcmo/Desktop/RutgersCourseMaterial/PrincipleOfAI/project/code/Metric_Computation.py:268: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  DI_up_mat = 100*np.divide(DM_up_SMA,TR_mat)\n",
      "/Users/vcmo/Desktop/RutgersCourseMaterial/PrincipleOfAI/project/code/Metric_Computation.py:269: RuntimeWarning: invalid value encountered in true_divide\n",
      "  DI_down_mat = 100*np.divide(DM_down_SMA,TR_mat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and processing stock data, 100.0% has been finished... finished...."
     ]
    }
   ],
   "source": [
    "# define the global variables\n",
    "holistic_stock_data_X = 0\n",
    "holistic_stock_data_X_prev = 0\n",
    "holistic_S_P_500_stock = 0\n",
    "holistic_S_P_500_stock_prev = 0\n",
    "holistic_target_value_array = 0\n",
    "holistic_target_value_array_prev = 0\n",
    "holistic_target_gradient_array = 0\n",
    "holistic_target_gradient_array_prev = 0\n",
    "holistic_price_change_flag = 0\n",
    "holistic_price_trend_flag = 0\n",
    "for cSymbol in range(len(stock_symbol_list)):\n",
    "    print('\\rReading and processing stock data, '+str(cSymbol*100/len(stock_symbol_list))+'% has been finished...',end=\"\",\n",
    "          flush=True)\n",
    "    try:\n",
    "        # retrieve and compute stock price info\n",
    "        this_stock_info = stock_data_read_and_process(stock_symbol=stock_symbol_list[cSymbol],\n",
    "                                                      query_key=alpha_van_key_1,\n",
    "                                                      stock_save_path=stock_info_save_path, \n",
    "                                                      start_period=1, \n",
    "                                                      end_period=25, \n",
    "                                                      S_P_500_date_array=S_P_500_date_array,\n",
    "                                                      S_P_500_value_array=S_P_500_value_array)\n",
    "    except:\n",
    "        print('\\nStock \\''+cSymbol+'\\' failed in query!\\n')\n",
    "        continue\n",
    "    # retrieve information from the tuple\n",
    "    # 10-returns:\n",
    "    # stock_data_X, ,stock_data_X_prev, S_P_500_stock, S_P_500_stock_prev,  \n",
    "    # target_value_array, target_value_array_prev, target_gradient_array, target_gradient_array_prev, \n",
    "    # inc_dec_flag_array, up_down_trend_flag_array\n",
    "    this_stock_data_X = this_stock_info[0]\n",
    "    this_stock_data_X_prev = this_stock_info[1]\n",
    "    this_S_P_500_stock = np.reshape(this_stock_info[2] ,[-1,1])\n",
    "    this_S_P_500_stock_prev = np.reshape(this_stock_info[3] ,[-1,1])\n",
    "    this_target_value_array = np.reshape(this_stock_info[4],[-1,1])\n",
    "    this_target_value_array_prev = np.reshape(this_stock_info[5],[-1,1])\n",
    "    this_target_gradient_array = np.reshape(this_stock_info[6],[-1,1])\n",
    "    this_target_gradient_array_prev = np.reshape(this_stock_info[7],[-1,1])\n",
    "    this_inc_dec_flag_array = np.reshape(this_stock_info[8],[-1,1])\n",
    "    this_up_down_trend_flag_array = np.reshape(this_stock_info[9],[-1,1])\n",
    "    if cSymbol==0:\n",
    "        holistic_stock_data_X = this_stock_data_X[:]\n",
    "        holistic_stock_data_X_prev = this_stock_data_X_prev[:]\n",
    "        holistic_S_P_500_stock = this_S_P_500_stock[:]\n",
    "        holistic_S_P_500_stock_prev = this_S_P_500_stock_prev[:]\n",
    "        holistic_target_value_array = this_target_value_array[:]\n",
    "        holistic_target_value_array_prev = this_target_value_array_prev[:]\n",
    "        holistic_target_gradient_array = this_target_gradient_array[:]\n",
    "        holistic_target_gradient_array_prev = this_target_gradient_array_prev[:]\n",
    "        holistic_price_change_flag = this_inc_dec_flag_array[:]\n",
    "        holistic_price_trend_flag = this_up_down_trend_flag_array[:]\n",
    "    else:\n",
    "        # 'current' arrays\n",
    "        holistic_stock_data_X = np.concatenate([holistic_stock_data_X, this_stock_data_X], axis=0)\n",
    "        holistic_S_P_500_stock = np.concatenate([holistic_S_P_500_stock, this_S_P_500_stock], axis=0)\n",
    "        holistic_target_value_array = np.concatenate([holistic_target_value_array, this_target_value_array], axis=0)\n",
    "        holistic_target_gradient_array = np.concatenate([holistic_target_gradient_array, this_target_gradient_array], axis=0)\n",
    "        # 'previous' arrays\n",
    "        holistic_stock_data_X_prev = np.concatenate([holistic_stock_data_X_prev, \n",
    "                                                     this_stock_data_X_prev], axis=0)\n",
    "        holistic_S_P_500_stock_prev = np.concatenate([holistic_S_P_500_stock_prev, \n",
    "                                                      this_S_P_500_stock_prev], axis=0)\n",
    "        holistic_target_value_array_prev = np.concatenate([holistic_target_value_array_prev, \n",
    "                                                           this_target_value_array_prev], axis=0)\n",
    "        holistic_target_gradient_array_prev = np.concatenate([holistic_target_gradient_array_prev, \n",
    "                                                              this_target_gradient_array_prev], axis=0)\n",
    "        holistic_price_change_flag = np.concatenate([holistic_price_change_flag,\n",
    "                                                     this_inc_dec_flag_array],axis=0)\n",
    "        holistic_price_trend_flag = np.concatenate([holistic_price_trend_flag,\n",
    "                                                    this_up_down_trend_flag_array],axis=0)\n",
    "    print('\\rReading and processing stock data, '+str((cSymbol+1)*100/len(stock_symbol_list))+'% has been finished...',end=\"\",\n",
    "          flush=True)\n",
    "    # avoid query more than 5 times per minute\n",
    "    time.sleep(12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240752, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(holistic_S_P_500_stock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save 'current' info\n",
    "np.save(stock_info_save_path+'data_X.npy',holistic_stock_data_X)\n",
    "np.save(stock_info_save_path+'SP500.npy',holistic_S_P_500_stock)\n",
    "np.save(stock_info_save_path+'target_value.npy',holistic_target_value_array)\n",
    "np.save(stock_info_save_path+'target_gradient.npy',holistic_target_gradient_array)\n",
    "np.save(stock_info_save_path+'price_change_flag.npy',holistic_price_change_flag)\n",
    "np.save(stock_info_save_path+'price_trend_flag.npy',holistic_price_trend_flag)\n",
    "# save 'prev' info\n",
    "np.save(stock_info_save_path+'data_X_prev.npy',holistic_stock_data_X_prev)\n",
    "np.save(stock_info_save_path+'SP500_prev.npy',holistic_S_P_500_stock_prev)\n",
    "np.save(stock_info_save_path+'target_value_prev.npy',holistic_target_value_array_prev)\n",
    "np.save(stock_info_save_path+'target_gradient_prev.npy',holistic_target_gradient_array_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
