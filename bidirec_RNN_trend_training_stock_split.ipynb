{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\iroooon\\anaconda3\\envs\\sci_computing\\lib\\importlib\\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "c:\\users\\iroooon\\anaconda3\\envs\\sci_computing\\lib\\importlib\\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from Metric_Computation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify the path to read\n",
    "data_load_path = '../data/intermediate_stock_split/'\n",
    "# read the data\n",
    "# train and validation\n",
    "data_train_X = np.load(data_load_path+'data_X_train.npy')\n",
    "data_train_X_prev = np.load(data_load_path+'data_X_prev_train.npy')\n",
    "SP500_train_X = np.load(data_load_path+'SP500_train.npy')\n",
    "SP500_train_X_prev = np.load(data_load_path+'SP500_prev_train.npy')\n",
    "value_target_train = np.load(data_load_path+'target_value_train.npy')\n",
    "value_target_train_prev = np.load(data_load_path+'target_value_prev_train.npy')\n",
    "gradient_target_train = np.load(data_load_path+'target_gradient_train.npy')\n",
    "gradient_target_train_prev = np.load(data_load_path+'target_gradient_prev_train.npy')\n",
    "trend_target_train = np.load(data_load_path+'price_trend_flag_train.npy')\n",
    "# test\n",
    "data_test_X = np.load(data_load_path+'data_X_test.npy')\n",
    "data_test_X_prev = np.load(data_load_path+'data_X_prev_test.npy')\n",
    "SP500_test_X = np.load(data_load_path+'SP500_test.npy')\n",
    "SP500_test_X_prev = np.load(data_load_path+'SP500_prev_test.npy')\n",
    "value_target_test = np.load(data_load_path+'target_value_test.npy')\n",
    "value_target_test_prev = np.load(data_load_path+'target_value_prev_test.npy')\n",
    "gradient_target_test = np.load(data_load_path+'target_gradient_test.npy')\n",
    "gradient_target_test_prev = np.load(data_load_path+'target_gradient_prev_test.npy')\n",
    "trend_target_test = np.load(data_load_path+'price_trend_flag_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training set: drop instance that contains inf values\n",
    "valid_series_index_train = np.where((np.sum(np.sum(data_train_X,axis=-1),axis=-1)!=np.inf)&((np.sum(np.sum(data_train_X_prev,axis=-1),axis=-1)!=np.inf)))[0]\n",
    "data_input_X = data_train_X[valid_series_index_train]\n",
    "data_input_X_prev = data_train_X_prev[valid_series_index_train]\n",
    "SP500_input_X = np.reshape(SP500_train_X[valid_series_index_train],[-1,1])\n",
    "SP500_input_X_prev = np.reshape(SP500_train_X_prev[valid_series_index_train],[-1,1])\n",
    "value_target = np.reshape(value_target_train[valid_series_index_train],[-1,1])\n",
    "value_target_prev = np.reshape(value_target_train_prev[valid_series_index_train],[-1,1])\n",
    "gradient_target = np.reshape(gradient_target_train[valid_series_index_train],[-1,1])\n",
    "gradient_target_prev = np.reshape(gradient_target_train_prev[valid_series_index_train],[-1,1])\n",
    "trend_target = np.reshape(trend_target_train[valid_series_index_train],[-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing set: drop instance that contains inf values\n",
    "valid_series_index_test = np.where((np.sum(np.sum(data_test_X,axis=-1),axis=-1)!=np.inf)&((np.sum(np.sum(data_test_X_prev,axis=-1),axis=-1)!=np.inf)))[0]\n",
    "data_test_X = data_test_X[valid_series_index_test]\n",
    "data_test_X_prev = data_test_X_prev[valid_series_index_test]\n",
    "SP500_test_X = np.reshape(SP500_test_X[valid_series_index_test],[-1,1])\n",
    "SP500_test_X_prev = np.reshape(SP500_test_X_prev[valid_series_index_test],[-1,1])\n",
    "value_target_test = np.reshape(value_target_test[valid_series_index_test],[-1,1])\n",
    "value_target_test_prev = np.reshape(value_target_test_prev[valid_series_index_test],[-1,1])\n",
    "gradient_target_test = np.reshape(gradient_target_test[valid_series_index_test],[-1,1])\n",
    "gradient_target_test_prev = np.reshape(gradient_target_test_prev[valid_series_index_test],[-1,1])\n",
    "trend_target_test = np.reshape(trend_target_test[valid_series_index_test],[-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the training data into \n",
    "ind_permutation = np.random.permutation(data_input_X.shape[0])\n",
    "# training and testing index\n",
    "training_ind = ind_permutation[:ind_permutation.shape[0]*2//3]\n",
    "validation_ind = ind_permutation[ind_permutation.shape[0]*2//3:]\n",
    "# indexing the data\n",
    "# time series data\n",
    "# training\n",
    "data_train_X = data_input_X[training_ind,:,:]\n",
    "data_train_X_prev = data_input_X_prev[training_ind,:,:]\n",
    "# validation\n",
    "data_valid_X = data_input_X[validation_ind,:,:]\n",
    "data_valid_X_prev = data_input_X[validation_ind,:,:]\n",
    "# SP500 info\n",
    "# train\n",
    "SP500_train_X = np.reshape(SP500_input_X[training_ind],[-1,1])\n",
    "SP500_train_X_prev = np.reshape(SP500_input_X_prev[training_ind],[-1,1])\n",
    "# validation\n",
    "SP500_valid_X = np.reshape(SP500_input_X[validation_ind],[-1,1])\n",
    "SP500_valid_X_prev = np.reshape(SP500_input_X_prev[validation_ind],[-1,1])\n",
    "# value prediction (for reference usage only)\n",
    "# train\n",
    "value_target_train = np.reshape(value_target[training_ind],[-1,1])\n",
    "value_target_train_prev = np.reshape(value_target_prev[training_ind],[-1,1])\n",
    "# validation\n",
    "value_target_valid = np.reshape(value_target[validation_ind],[-1,1])\n",
    "value_target_valid_prev = np.reshape(value_target_prev[validation_ind],[-1,1])\n",
    "# gradient prediction (the real 'labels' for training)\n",
    "# train\n",
    "gradient_target_train = np.reshape(gradient_target[training_ind],[-1,1])\n",
    "gradient_target_train_prev = np.reshape(gradient_target_prev[training_ind],[-1,1])\n",
    "# validation\n",
    "gradient_target_valid = np.reshape(gradient_target[validation_ind],[-1,1])\n",
    "gradient_target_valid_prev = np.reshape(gradient_target_prev[validation_ind],[-1,1])\n",
    "# trend info\n",
    "# train\n",
    "trend_target_train = np.reshape(trend_target[training_ind],[-1,1])\n",
    "# validation\n",
    "trend_target_valid = np.reshape(trend_target[validation_ind],[-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_and_future_price_train = np.concatenate([value_target_train_prev, value_target_train],axis=1)\n",
    "current_and_future_price_valid = np.concatenate([value_target_valid_prev, value_target_valid],axis=1)\n",
    "current_and_future_price_test = np.concatenate([value_target_test_prev, value_target_test],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preceed the trend(class) target to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding_label(label_input, num_class=None):\n",
    "    '''\n",
    "    :param label_input: The sparse form of input label (2,0,1,3,0,1,2etc.)\n",
    "    :param num_class: The number of classes, if keep None, then automatically infer from the given label input\n",
    "    '''\n",
    "    # retrieve the number of input data\n",
    "    nData = label_input.shape[0]\n",
    "    # reshape the data\n",
    "    label_input_flat = np.reshape(label_input, [-1])\n",
    "    if (label_input_flat.shape[0]!=nData):            # which means the input label is not 'mathematically 1-d'\n",
    "        raise ValueError('The input label must be 1-d mathematically')\n",
    "    # infer the number of class if input is None\n",
    "    if num_class is None:\n",
    "        num_class = (int)(np.amax(label_input)+1)\n",
    "    # create the return encoded matrx\n",
    "    one_hot_label_mat = np.zeros([nData, num_class])\n",
    "    # get a row index to assist the batch-assigning\n",
    "    row_ind_batch = np.arange(nData)\n",
    "    # assign '1's to the corresponding positions\n",
    "    one_hot_label_mat[row_ind_batch, label_input_flat.astype('int')] = 1\n",
    "    \n",
    "    return one_hot_label_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trend_target_train_input = one_hot_encoding_label(trend_target_train)\n",
    "trend_target_valid_input = one_hot_encoding_label(trend_target_valid)\n",
    "trend_target_test_input = one_hot_encoding_label(trend_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# confusion metrics\n",
    "def classification_info_computation(pred_label, true_label, num_class):\n",
    "    '''\n",
    "    :param pred_label: the sparse (not one-hot) prediction of labels\n",
    "    :param true_label: the sparse (not one-hot) ground-truth of labels\n",
    "    :param num_class: number of classes\n",
    "    '''\n",
    "    # flatten the two label arrays if they are not already so\n",
    "    pred_label = np.reshape(pred_label,[-1])\n",
    "    true_label = np.reshape(true_label,[-1])\n",
    "    # initialize the confusion maxtrix array\n",
    "    class_matrix = np.zeros([num_class, num_class])    # each row is the true labels\n",
    "    # initialize the precision and recall arrays\n",
    "    precision_array = np.zeros([num_class])\n",
    "    recall_array = np.zeros([num_class])\n",
    "    # fill the confusion-prediction matrix\n",
    "    for cClass_True in range(num_class):\n",
    "        # retrieve the current \n",
    "        current_cClass_ind = np.where(true_label==cClass_True)[0]\n",
    "        # retrueve the corresponding predictions\n",
    "        current_cClass_pred = pred_label[current_cClass_ind]\n",
    "        # fill the evaluation matrx\n",
    "        for cClass_Pred in range(num_class):\n",
    "            cClass_pred_num = np.where(current_cClass_pred==cClass_Pred)[0].shape[0]\n",
    "            class_matrix[cClass_True, cClass_Pred] = cClass_pred_num\n",
    "    # fill the precision and recall arrays\n",
    "    for cClass_True in range(num_class):\n",
    "        precision_array[cClass_True] = class_matrix[cClass_True,cClass_True]/np.sum(class_matrix[:,cClass_True])\n",
    "        recall_array[cClass_True] = class_matrix[cClass_True,cClass_True]/np.sum(class_matrix[cClass_True,:])\n",
    "        \n",
    "    return class_matrix, precision_array, recall_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'regret' function\n",
    "def invest_regret_comput(pred_label, true_label):\n",
    "    '''\n",
    "    The function to return the 'regret' defined by the real investment scenarios\n",
    "    :param pred_label: the sparse (not one-hot) prediction of labels\n",
    "    :param true_label: the sparse (not one-hot) ground-truth of labels\n",
    "    With the meaning 2=uptrend 1=downtrend 0=non-trend\n",
    "    Strategy: \n",
    "        predict 0: don't buy or sell\n",
    "        predict 1: sell\n",
    "        predict 2: buy\n",
    "    ******************** Truth Table *********************\n",
    "    | True Label | Predicted Label | Regret |\n",
    "    |      0     |        0        |   0    |\n",
    "    |      0     |        1        |   1    |\n",
    "    |      0     |        2        |   1    |\n",
    "    |      1     |        0        |   1    |\n",
    "    |      1     |        1        |   0    |\n",
    "    |      1     |        2        |   2    |\n",
    "    |      2     |        0        |   1    |\n",
    "    |      2     |        1        |   2    |\n",
    "    |      2     |        2        |   0    |\n",
    "    '''\n",
    "    # flatten the two label arrays if they are not already so\n",
    "    pred_label = np.reshape(pred_label,[-1])\n",
    "    true_label = np.reshape(true_label,[-1])\n",
    "    # check if the two arrays are of the same legth\n",
    "    if pred_label.shape[0]!=true_label.shape[0]:\n",
    "        raise ValueError('The predicted and the true labels must be in the same length!')\n",
    "    # placeholder of regret array\n",
    "    regret_array = np.zeros([pred_label.shape[0]])\n",
    "    # check the conditions for regret '1'\n",
    "    one_regret_ind = np.where(((true_label==0)&(pred_label==2))|((true_label==0)&(pred_label==1))|((true_label==2)&(pred_label==0))|((true_label==1)&(pred_label==0)))[0]\n",
    "    # check the conditions for regret '2'\n",
    "    two_regret_ind = np.where(((true_label==1)&(pred_label==2))|((true_label==2)&(pred_label==1)))[0]\n",
    "    # assign regret values to the entries\n",
    "    regret_array[one_regret_ind] = 1.0\n",
    "    regret_array[two_regret_ind] = 2.0\n",
    "    # compute the overall regret\n",
    "    overall_regret = np.mean(regret_array)\n",
    "    \n",
    "    return overall_regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters of the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rnn cell sequence length and dimensions\n",
    "nChunck = data_input_X.shape[1]\n",
    "nDim = data_input_X.shape[2]\n",
    "# output dimension\n",
    "nClass = 3\n",
    "# number of nested RNN stack\n",
    "nStack_rnn = 5\n",
    "# rnn dimension\n",
    "rnn_size = 64\n",
    "# training hyper-parameter\n",
    "nEpochs = 100\n",
    "batch_size = 256\n",
    "pred_loss_coeff = 1e-3\n",
    "l2_FC_pen_coeff = 1e-3\n",
    "fluc_pen_coeff = 1e-4\n",
    "learning_rate = 1e-4\n",
    "tol = 1e-3\n",
    "focal_gamma = 3.0     # parameter for the focal loss\n",
    "trend_weight_alpha = 2.0  # parameter to encourage output 1/2\n",
    "# data info for feeding them\n",
    "nData_train = data_train_X.shape[0]\n",
    "nData_test = data_test_X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A 'Erease Graph' here to enable us to re-run things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Model_path = '../model/trend_bidirec_RNN_stock_split/'\n",
    "#Model saving function\n",
    "def tf_save_model(session):\n",
    "    if not os.path.exists(Model_path):\n",
    "        os.makedirs(Model_path)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(session, Model_path+'Bidirec_RNN.checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the RNN graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the x input as float and reshape input\n",
    "x_time_series = tf.placeholder(\"float\",[None,nChunck,nDim])       # Batch_size * n_chunk * chunk_size\n",
    "x_time_series_prev = tf.placeholder(\"float\",[None,nChunck,nDim])  # Batch_size * n_chunk * chunk_size\n",
    "x_SP_data = tf.placeholder(\"float\",[None,1])                      # Batch_size * 1\n",
    "x_SP_data_prev = tf.placeholder(\"float\",[None,1])                 # Batch_size * 1\n",
    "x_current_price = tf.placeholder(\"float\",[None,2])                # Batch_size * 2\n",
    "y = tf.placeholder(\"float\",[None,1])                              # Batch_size * 1\n",
    "y_prev = tf.placeholder(\"float\",[None,1])                         # Batch_size * 1\n",
    "y_trend = tf.placeholder(\"float\",[None,nClass])                   # Batch_size * n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def swish_activation(data_input):\n",
    "    '''\n",
    "    Swish activation, by Ramachandran et al., Google Brain, 2017\n",
    "    The activation is found by a RNN-based combinational search, and it has been proved consistently outperforming\n",
    "        RELU on networks like NASNet-A and Inception-ResNet-v2\n",
    "    '''\n",
    "    # define the beta variable as trainable\n",
    "    para_beta = tf.Variable(tf.random_uniform([1]))\n",
    "    swish_output = data_input*tf.sigmoid(para_beta*data_input)\n",
    "    \n",
    "    return swish_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Recurrent_neural_network(x_time_series, x_SP_data, x_time_series_prev, x_SP_data_prev, x_current_price):\n",
    "    # process the data concatenating the current and previous data\n",
    "    data_processed = tf.concat([x_time_series,x_time_series_prev],axis=0)\n",
    "    # split the data into (nChunck * [batch_size, nDim])       \n",
    "    data_processed = tf.split(data_processed, num_or_size_splits=nChunck, axis=1)  # (nChunck * [batch_size, nDim])\n",
    "    data_processed = [tf.squeeze(this_time_data, axis=1) for this_time_data in data_processed]\n",
    "    # define individual GRU cells\n",
    "    # forward\n",
    "    stack_GRU_cell_forward = []\n",
    "    for _ in range(nStack_rnn):\n",
    "        current_rnn_cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.GRUCell(rnn_size),\n",
    "                                                         input_keep_prob=0.7,\n",
    "                                                         variational_recurrent=False)# ,\n",
    "#                                                          input_size=nDim,\n",
    "#                                                          dtype=tf.float32)\n",
    "        stack_GRU_cell_forward.append(current_rnn_cell)\n",
    "    heiarchy_RNN_forward = tf.contrib.rnn.MultiRNNCell(stack_GRU_cell_forward)\n",
    "    # backward\n",
    "    stack_GRU_cell_backward = []\n",
    "    for _ in range(nStack_rnn):\n",
    "        current_rnn_cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.GRUCell(rnn_size),\n",
    "                                                         input_keep_prob=0.7,\n",
    "                                                         variational_recurrent=False)#,\n",
    "#                                                          input_size=nDim,\n",
    "#                                                          dtype=tf.float32)\n",
    "        stack_GRU_cell_backward.append(current_rnn_cell)\n",
    "    heiarchy_RNN_backward = tf.contrib.rnn.MultiRNNCell(stack_GRU_cell_backward)\n",
    "    # output of the forward-backword RNN\n",
    "    outputs_all, output_for_final, output_back_final = tf.contrib.rnn.static_bidirectional_rnn(cell_fw=heiarchy_RNN_forward,\n",
    "                                                                                               cell_bw=heiarchy_RNN_backward,\n",
    "                                                                                               inputs=data_processed,\n",
    "                                                                                               dtype=tf.float32)\n",
    "    # Post-RNN process\n",
    "    # concatenate the current and prev S&P-500\n",
    "    x_SP_data_processed = tf.concat([x_SP_data,x_SP_data_prev],axis=0)\n",
    "    # [nBatch x 2*rnn_stack*rnn_size+1]\n",
    "    # print(outputs_all[-1].get_shape().as_list())\n",
    "    out_status_processed = tf.concat([outputs_all[-1], x_SP_data_processed],axis=1)   # [batch_size x (2*rnn_size+1)]\n",
    "    with tf.name_scope(\"FC_layers_rate\"):\n",
    "        weight_FC_rate = {'weight_hidden': tf.Variable(tf.random_normal([2*rnn_size+1, 64])),\n",
    "                     'weight_out': tf.Variable(tf.random_normal([64, 1]))}\n",
    "    with tf.name_scope(\"FC_layers_scale\"):\n",
    "        weight_FC_scale = {'weight_hidden': tf.Variable(tf.random_normal([2*rnn_size+1, 64])),\n",
    "                     'weight_out': tf.Variable(tf.random_normal([64, 1]))}\n",
    "    with tf.name_scope(\"FC_layers_class\"):\n",
    "        weight_FC_class = {'weight_hidden_1': tf.Variable(tf.random_normal([2*rnn_size+4, 128])),\n",
    "                           'weight_hidden_2': tf.Variable(tf.random_normal([128, 64])),\n",
    "                           'weight_out': tf.Variable(tf.random_normal([64, nClass]))}\n",
    "    # ***************rate prediction**************\n",
    "    # fully-connected hidden\n",
    "    process_output_rate = tf.matmul(out_status_processed, weight_FC_rate[\"weight_hidden\"])    # [batch_size x 64]\n",
    "    # non-linear activation \n",
    "    process_output_rate = swish_activation(process_output_rate)\n",
    "    process_output_rate = tf.nn.dropout(process_output_rate, keep_prob = 0.7)\n",
    "    #         process_output = tf.nn.tanh(process_output)\n",
    "    # process to the final output\n",
    "    final_output_rate = tf.nn.sigmoid(tf.matmul(process_output_rate, weight_FC_rate[\"weight_out\"]))  # [batch_size x 1]\n",
    "    # ***************scale prediction**************\n",
    "    process_output_scale = tf.matmul(out_status_processed, weight_FC_scale[\"weight_hidden\"])\n",
    "    # non-linear activation \n",
    "    process_output_scale = swish_activation(process_output_scale)\n",
    "    process_output_scale = tf.nn.dropout(process_output_scale, keep_prob = 0.7)\n",
    "    # out\n",
    "    # pass to RELU to limit the learning inside the regime of scaling\n",
    "    final_output_scale = tf.nn.relu(tf.matmul(process_output_scale, weight_FC_scale[\"weight_out\"]))\n",
    "    # combine them to get the final output\n",
    "    final_output = tf.multiply(final_output_rate, final_output_scale)\n",
    "    # split the results to get the prediction for current and previous data\n",
    "    current_prediction, prev_prediction = tf.split(final_output,num_or_size_splits=2,axis=0)\n",
    "    # ***************class prediction****************\n",
    "    out_status_processed_class = tf.concat([tf.split(out_status_processed,num_or_size_splits=2,axis=0)[0],  # only pick the 'current'\n",
    "                                            x_current_price,\n",
    "                                            current_prediction],axis=1)\n",
    "    # layer 1\n",
    "    process_output_class = tf.matmul(out_status_processed_class, weight_FC_class[\"weight_hidden_1\"])\n",
    "    process_output_class = swish_activation(process_output_class)\n",
    "    # layer 2\n",
    "    process_output_class = tf.matmul(process_output_class, weight_FC_class[\"weight_hidden_2\"])\n",
    "    process_output_class = swish_activation(process_output_class)\n",
    "    # layer 3 (out-layer)\n",
    "    final_output_class = tf.matmul(process_output_class, weight_FC_class[\"weight_out\"])            \n",
    "\n",
    "    return current_prediction, prev_prediction, final_output_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def focal_classification_loss(logits, labels, trend_weight):\n",
    "    '''\n",
    "    will be resemble the cross-entropy calssification loss, but with the focal loss-decrease term for well-classes \n",
    "    examples to tackle the problem of imbalance data.\n",
    "    :param logits: The [nData * nClass] un-normalised prediction\n",
    "    :param labels: The [nData * nClass] one-hot true labels\n",
    "    :param trend_weight: To encourage the network to predict up/down trends. Should be specify\n",
    "    :return: the loss of [nData] shape\n",
    "    ''' \n",
    "    # get the pobabilistic sotmax output\n",
    "    soft_max_prediction = tf.nn.softmax(logits, axis=1)\n",
    "    # process to log(soft_max) output, use this integrated function to faciliate numerical stability\n",
    "    log_soft_max_prediction = tf.nn.log_softmax(logits, axis=1)\n",
    "    # compute focal loss\n",
    "    class_weight = tf.constant([1.0, trend_weight, trend_weight])\n",
    "    # -sum(y*(1-p)^(gamma)*log(p))  \n",
    "    focal_loss = -tf.reduce_sum(tf.multiply(tf.multiply(labels,tf.multiply(tf.pow(1-soft_max_prediction,focal_gamma),\n",
    "                                                                           log_soft_max_prediction)),class_weight),\n",
    "                                axis=1)\n",
    "    \n",
    "    return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_recurrent_neural_network(x_time_series, x_SP_data, x_time_series_prev, x_SP_data_prev, x_current_price):\n",
    "    # get the prediction\n",
    "    grad_pred, grad_pred_prev, class_pred = Recurrent_neural_network(x_time_series, x_SP_data, x_time_series_prev, x_SP_data_prev, x_current_price)\n",
    "    # mean square loss\n",
    "    loss_prediction = tf.losses.mean_squared_error(predictions=grad_pred, labels=y)\n",
    "    # compute the loss of fluctuation\n",
    "    # use this scheme to back-prop gradient to both sides\n",
    "    loss_fluctuation = (tf.losses.mean_squared_error(predictions=grad_pred, labels=grad_pred_prev) + \n",
    "                        tf.losses.mean_squared_error(predictions=grad_pred_prev, labels=grad_pred))/2\n",
    "    # mis-classification loss\n",
    "    # loss_classification = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=class_pred, labels=y_trend))\n",
    "    # focal loss\n",
    "    loss_classification = tf.reduce_mean(focal_classification_loss(logits=class_pred, labels=y_trend, trend_weight=trend_weight_alpha))\n",
    "    # l_2 loss for FC layers\n",
    "    reg_vars = (tf.trainable_variables(scope='FC_layers_rate') \n",
    "                + tf.trainable_variables(scope='FC_layers_scale') \n",
    "                + tf.trainable_variables(scope='FC_layers_class'))\n",
    "    loss_l2_FC = tf.add_n([tf.nn.l2_loss(var) for var in reg_vars])\n",
    "    # overall loss\n",
    "    loss = loss_classification + pred_loss_coeff*loss_prediction + l2_FC_pen_coeff*loss_l2_FC + fluc_pen_coeff*loss_fluctuation\n",
    "    # defineing optimiser\n",
    "    optimiser_org = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    # apply gradient clipping to avoid gradient exploding\n",
    "    gradients_nn = optimiser_org.compute_gradients(loss)\n",
    "    clipped_grad = [(tf.clip_by_value(grad, -10., 10.), var) for grad, var in gradients_nn]\n",
    "    optimiser = optimiser_org.apply_gradients(clipped_grad)\n",
    "    prev_test_loss = 0\n",
    "    # evaluation tensor\n",
    "    label_flatten_tensor = tf.argmax(y_trend, 1)\n",
    "    logit_flatten_tensor = tf.argmax(class_pred,1)\n",
    "    acc_tensor = tf.reduce_mean(tf.cast(tf.equal(label_flatten_tensor, logit_flatten_tensor),tf.float32))\n",
    "    # the training dictionary\n",
    "    train_dict = {x_time_series: data_train_X,\n",
    "                  x_time_series_prev: data_train_X_prev,\n",
    "                  x_SP_data: SP500_train_X,\n",
    "                  x_SP_data_prev: SP500_train_X_prev,\n",
    "                  y: gradient_target_train,\n",
    "                  y_prev: gradient_target_train_prev, \n",
    "                  y_trend: trend_target_train_input,\n",
    "                  x_current_price: current_and_future_price_train}\n",
    "    # the validation dictionary\n",
    "    valid_dict = {x_time_series: data_valid_X,\n",
    "                 x_time_series_prev: data_valid_X_prev,\n",
    "                 x_SP_data: SP500_valid_X,\n",
    "                 x_SP_data_prev: SP500_valid_X_prev,\n",
    "                 y: gradient_target_valid,\n",
    "                 y_prev: gradient_target_valid_prev, \n",
    "                 y_trend: trend_target_valid_input, \n",
    "                 x_current_price: current_and_future_price_valid}\n",
    "    # the test dictionary\n",
    "    test_dict = {x_time_series: data_test_X,\n",
    "                 x_time_series_prev: data_test_X_prev,\n",
    "                 x_SP_data: SP500_test_X,\n",
    "                 x_SP_data_prev: SP500_test_X_prev,\n",
    "                 y: gradient_target_test,\n",
    "                 y_prev: gradient_target_test_prev, \n",
    "                 y_trend: trend_target_test_input, \n",
    "                 x_current_price: current_and_future_price_test}\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # define the lists to collect the infomation while training\n",
    "        # train\n",
    "        train_precision_epoch_collection = []\n",
    "        train_recall_epoch_collection = []\n",
    "        train_regret_epoch_collection = []\n",
    "        # valid\n",
    "        valid_precision_epoch_collection = []\n",
    "        valid_recall_epoch_collection = []\n",
    "        valid_regret_epoch_collection = []\n",
    "        # test\n",
    "        test_precision_epoch_collection = []\n",
    "        test_recall_epoch_collection = []\n",
    "        test_regret_epoch_collection = []\n",
    "        for cEpoch in range(nEpochs):\n",
    "            current_Epoch_Loss = 0\n",
    "            random_index = np.random.choice(nData_train, size=nData_train)    # Use this to avoid overflow for the derivative index\n",
    "            for i in range(nData_train//batch_size):\n",
    "                this_index = random_index[i*batch_size:(i+1)*batch_size]\n",
    "                current_x_time_series = data_train_X[this_index]\n",
    "                current_x_time_series_prev = data_train_X_prev[this_index]\n",
    "                current_x_SP_500 = SP500_train_X[this_index]\n",
    "                current_x_SP_500_prev = SP500_train_X_prev[this_index]\n",
    "                current_x_price_info = current_and_future_price_train[this_index]\n",
    "                current_y_gradient = gradient_target_train[this_index]\n",
    "                current_y_gradient_prev = gradient_target_train_prev[this_index]\n",
    "                current_y_trend = trend_target_train_input[this_index]\n",
    "                #Get how many batches we need for each epoch\n",
    "                this_feed_dict = {x_time_series: current_x_time_series,\n",
    "                                  x_time_series_prev: current_x_time_series_prev,\n",
    "                                  x_SP_data: current_x_SP_500, \n",
    "                                  x_SP_data_prev: current_x_SP_500_prev,\n",
    "                                  y: current_y_gradient, \n",
    "                                  y_prev: current_y_gradient_prev,\n",
    "                                  y_trend: current_y_trend, \n",
    "                                  x_current_price: current_x_price_info}\n",
    "                _, currentloss = sess.run([optimiser,loss], feed_dict = this_feed_dict)\n",
    "#                 print(currentloss)\n",
    "#                 print(np.amax(current_x_time_series))\n",
    "#                 print(np.amin(current_x_time_series))\n",
    "                current_Epoch_Loss += currentloss\n",
    "                # break\n",
    "            print('The',cEpoch+1,'th out of ',nEpochs,'Epochs in total has finished and loss in this epoch is',current_Epoch_Loss)\n",
    "            # Check the classification loss for training and testing data\n",
    "            train_data_loss = loss_classification.eval(feed_dict=train_dict)\n",
    "            valid_data_loss = loss_classification.eval(feed_dict=valid_dict)\n",
    "            test_data_loss = loss_classification.eval(feed_dict=test_dict)\n",
    "            print('The classification loss: training: ',train_data_loss, ' validation: ', valid_data_loss, ' test: ', test_data_loss)\n",
    "            # Check the regression loss for training and testing data\n",
    "            train_data_loss_reg = loss_prediction.eval(feed_dict=train_dict)\n",
    "            valid_data_loss_reg = loss_prediction.eval(feed_dict=valid_dict)\n",
    "            test_data_loss_reg = loss_prediction.eval(feed_dict=test_dict)\n",
    "            print('The regression loss: training: ',train_data_loss_reg, ' validation: ', valid_data_loss_reg, ' test: ', test_data_loss_reg)\n",
    "            # Check the fluctuation loss for training and testing data\n",
    "            train_data_loss_fluc = loss_fluctuation.eval(feed_dict=train_dict)\n",
    "            valid_data_loss_fluc = loss_fluctuation.eval(feed_dict=valid_dict)\n",
    "            test_data_loss_fluc = loss_fluctuation.eval(feed_dict=test_dict)\n",
    "            print('The fluctuation loss: training: ',train_data_loss_fluc, ' validation: ', valid_data_loss_fluc, ' test: ', test_data_loss_fluc)\n",
    "            # Also check the current classification accuracy\n",
    "            train_data_accuracy = acc_tensor.eval(feed_dict=train_dict)\n",
    "            valid_data_accuracy = acc_tensor.eval(feed_dict=valid_dict)\n",
    "            test_data_accuracy = acc_tensor.eval(feed_dict=test_dict)\n",
    "            # evluation and accuracy\n",
    "            # flattened predictions\n",
    "            # train\n",
    "            train_logit_flatten_numerical = logit_flatten_tensor.eval(train_dict)\n",
    "            train_label_flatten_numerical = label_flatten_tensor.eval(train_dict)\n",
    "            # validation\n",
    "            valid_logit_flatten_numerical = logit_flatten_tensor.eval(valid_dict)\n",
    "            valid_label_flatten_numerical = label_flatten_tensor.eval(valid_dict)\n",
    "            # test\n",
    "            test_logit_flatten_numerical = logit_flatten_tensor.eval(test_dict)\n",
    "            test_label_flatten_numerical = label_flatten_tensor.eval(test_dict)\n",
    "            # train\n",
    "            class_matrix_train, precision_array_train, recall_array_train = classification_info_computation(pred_label = train_logit_flatten_numerical, \n",
    "                                                                                                            true_label = train_label_flatten_numerical, \n",
    "                                                                                                            num_class=3)\n",
    "            down_mis_up_rate_train = class_matrix_train[2,1]/np.sum(class_matrix_train[:,1])\n",
    "            up_mis_down_rate_train = class_matrix_train[1,2]/np.sum(class_matrix_train[:,2])\n",
    "            precision_downtrend_adjusted_train = precision_array_train[1] - down_mis_up_rate_train\n",
    "            precision_uptrend_adjuest_train = precision_array_train[2] - up_mis_down_rate_train\n",
    "            # validation\n",
    "            class_matrix_valid, precision_array_valid, recall_array_valid = classification_info_computation(pred_label = valid_logit_flatten_numerical, \n",
    "                                                                                                            true_label = valid_label_flatten_numerical, \n",
    "                                                                                                            num_class=3)\n",
    "            down_mis_up_rate_valid = class_matrix_valid[2,1]/np.sum(class_matrix_valid[:,1])\n",
    "            up_mis_down_rate_valid = class_matrix_valid[1,2]/np.sum(class_matrix_valid[:,2])\n",
    "            precision_downtrend_adjusted_valid = precision_array_valid[1] - down_mis_up_rate_valid\n",
    "            precision_uptrend_adjuest_valid = precision_array_valid[2] - up_mis_down_rate_valid\n",
    "            # test\n",
    "            class_matrix_test, precision_array_test, recall_array_test = classification_info_computation(pred_label = test_logit_flatten_numerical, \n",
    "                                                                                                         true_label = test_label_flatten_numerical, \n",
    "                                                                                                         num_class=3)\n",
    "            down_mis_up_rate_test = class_matrix_test[2,1]/np.sum(class_matrix_test[:,1])\n",
    "            up_mis_down_rate_test = class_matrix_test[1,2]/np.sum(class_matrix_test[:,2])\n",
    "            precision_downtrend_adjusted_test = precision_array_test[1] - down_mis_up_rate_test\n",
    "            precision_uptrend_adjuest_test = precision_array_test[2] - up_mis_down_rate_test\n",
    "            # training and testing regret\n",
    "            regret_epoch_train = invest_regret_comput(pred_label = train_logit_flatten_numerical,\n",
    "                                                      true_label = train_label_flatten_numerical)\n",
    "            regret_epoch_valid = invest_regret_comput(pred_label = valid_logit_flatten_numerical,\n",
    "                                                      true_label = valid_label_flatten_numerical)\n",
    "            regret_epoch_test = invest_regret_comput(pred_label = test_logit_flatten_numerical,\n",
    "                                                     true_label = test_label_flatten_numerical)\n",
    "            # collect the information\n",
    "            # train\n",
    "            train_precision_epoch_collection.append(precision_array_train) \n",
    "            train_recall_epoch_collection.append(recall_array_train)\n",
    "            train_regret_epoch_collection.append(regret_epoch_train)\n",
    "            # validation\n",
    "            valid_precision_epoch_collection.append(precision_array_valid)\n",
    "            valid_recall_epoch_collection.append(recall_array_valid)\n",
    "            valid_regret_epoch_collection.append(regret_epoch_valid)\n",
    "            # test\n",
    "            test_precision_epoch_collection.append(precision_array_test)\n",
    "            test_recall_epoch_collection.append(recall_array_test)\n",
    "            test_regret_epoch_collection.append(regret_epoch_test)\n",
    "            # print out the information\n",
    "            print('The classification accuracy, train: ', train_data_accuracy, ' validation: ', valid_data_accuracy,' test: ', test_data_accuracy)\n",
    "            print('The train regret of the current epoch is: ', regret_epoch_train)\n",
    "            print('The test regret of the current epoch is: ', regret_epoch_test)\n",
    "            print('Recall Info:')\n",
    "            print('On the training data, the recall of Non-trend:', recall_array_train[0], ' Downtrend:', recall_array_train[1], ' Uptrend:', recall_array_train[2])\n",
    "            print('On the validation data, the recall of Non-trend:', recall_array_valid[0], ' Downtrend:', recall_array_valid[1], ' Uptrend:', recall_array_valid[2])\n",
    "            print('On the testing data, the recall of Non-trend:', recall_array_test[0], ' Downtrend:', recall_array_test[1], ' Uptrend:', recall_array_test[2])\n",
    "            print('Precision Info:')\n",
    "            print('On the training data, the precision of Non-trend:', precision_array_train[0], ' Downtrend:', precision_array_train[1], ' Uptrend:', precision_array_train[2])\n",
    "            print('On the validation data, the precision of Non-trend:', precision_array_valid[0], ' Downtrend:', precision_array_valid[1], ' Uptrend:', precision_array_valid[2])\n",
    "            print('On the testing data, the precision of Non-trend:', precision_array_test[0], ' Downtrend:', precision_array_test[1], ' Uptrend:', precision_array_test[2])\n",
    "            print('Adjusted Precision:')\n",
    "            print('On the training data, Adjusted Downtrend precision: ', precision_downtrend_adjusted_train, 'Adjusted Uptrend precision: ', precision_uptrend_adjuest_train)\n",
    "            print('On the validation data, Adjusted Downtrend precision: ', precision_downtrend_adjusted_valid, 'Adjusted Uptrend precision: ', precision_uptrend_adjuest_valid)\n",
    "            print('On the testing data, Adjusted Downtrend precision: ', precision_downtrend_adjusted_test, 'Adjusted Uptrend precision: ', precision_uptrend_adjuest_test)\n",
    "            print('**************************I\\'m the Divider*****************************')\n",
    "            if(abs((prev_test_loss-currentloss)/currentloss)<=tol):\n",
    "                print('Network stucked to local minimum!')\n",
    "                tf_save_model(sess)\n",
    "                break\n",
    "            # recall of three trends to determine convergence\n",
    "            if (precision_array_valid[0]>=0.50)and(recall_array_valid[1]>=0.15)and(recall_array_valid[2]>=0.15):\n",
    "                save_class_matrix_train = class_matrix_train[:]\n",
    "                save_class_matrix_valid = class_matrix_valid[:]\n",
    "                save_class_matrix_test = class_matrix_test[:]\n",
    "                tf_save_model(sess)\n",
    "                if regret_epoch_train<=0.60:\n",
    "                    print('Network converged!')\n",
    "                    break\n",
    "            # precision of the non-trend as the convergence critaria\n",
    "            downtrend_satis_condition = abs(precision_array_valid[1]-precision_downtrend_adjusted_valid)/abs(precision_array_valid[1])<1\n",
    "            uptrend_satis_condition = abs(precision_array_valid[2]-precision_uptrend_adjuest_valid)/abs(precision_array_valid[2])<1\n",
    "            if (recall_array_valid[0]>=0.50)and((downtrend_satis_condition)or(uptrend_satis_condition)):\n",
    "                save_class_matrix_train = class_matrix_train[:]\n",
    "                save_class_matrix_valid = class_matrix_valid[:]\n",
    "                save_class_matrix_test = class_matrix_test[:]\n",
    "                tf_save_model(sess)\n",
    "                if regret_epoch_train<=0.60:\n",
    "                    print('Network converged!')\n",
    "                    break\n",
    "            if cEpoch == (nEpochs-1):\n",
    "                print('Network failed to converge!')\n",
    "            prev_test_loss = currentloss\n",
    "    # make the lists as arrays\n",
    "    # train\n",
    "    train_precision_epoch_array = np.array(train_precision_epoch_collection)\n",
    "    train_recall_epoch_array = np.array(train_recall_epoch_collection)\n",
    "    train_regret_epoch_array = np.array(train_regret_epoch_collection)\n",
    "    # put them into one list to save \n",
    "    train_epoch_wise_info = [train_precision_epoch_array, train_recall_epoch_array, train_regret_epoch_array, save_class_matrix_train]\n",
    "    # validation\n",
    "    valid_precision_epoch_array = np.array(valid_precision_epoch_collection)\n",
    "    valid_recall_epoch_array = np.array(valid_recall_epoch_collection)\n",
    "    valid_regret_epoch_array = np.array(valid_regret_epoch_collection)\n",
    "    # put them into one list to save \n",
    "    valid_epoch_wise_info = [valid_precision_epoch_array, valid_recall_epoch_array, valid_regret_epoch_array, save_class_matrix_valid]\n",
    "    # test\n",
    "    test_precision_epoch_array = np.array(test_precision_epoch_collection)\n",
    "    test_recall_epoch_array = np.array(test_recall_epoch_collection)\n",
    "    test_regret_epoch_array = np.array(test_regret_epoch_collection)\n",
    "    # put them into one list to save\n",
    "    test_epoch_wise_info = [test_precision_epoch_array, test_recall_epoch_array, test_regret_epoch_array, save_class_matrix_test]\n",
    "    \n",
    "    return train_epoch_wise_info, valid_epoch_wise_info, test_epoch_wise_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1 th out of  100 Epochs in total has finished and loss in this epoch is 65572170.26953125\n",
      "The classification loss: training:  7824.9663  validation:  7623.508  test:  6333.705\n",
      "The regression loss: training:  53565464.0  validation:  53431870.0  test:  54099544.0\n",
      "The fluctuation loss: training:  61058990.0  validation:  60667680.0  test:  61588476.0\n",
      "The classification accuracy, train:  0.38951525  validation:  0.38961986  test:  0.38212466\n",
      "The train regret of the current epoch is:  0.708711847814364\n",
      "The test regret of the current epoch is:  0.7171826754568973\n",
      "Recall Info:\n",
      "On the training data, the recall of Non-trend: 0.404248973969354  Downtrend: 0.11492482129652452  Uptrend: 0.48651599197450723\n",
      "On the validation data, the recall of Non-trend: 0.40416441165579514  Downtrend: 0.11772732911549436  Uptrend: 0.4857025904917841\n",
      "On the testing data, the recall of Non-trend: 0.3631721670897761  Downtrend: 0.12059477812902471  Uptrend: 0.5598444045481747\n",
      "Precision Info:\n",
      "On the training data, the precision of Non-trend: 0.5866496300723368  Downtrend: 0.130465197621546  Uptrend: 0.2856771600339576\n",
      "On the validation data, the precision of Non-trend: 0.5846794132190539  Downtrend: 0.13182386442561467  Uptrend: 0.28838275119783346\n",
      "On the testing data, the precision of Non-trend: 0.5875571735274899  Downtrend: 0.16709928617780662  Uptrend: 0.2894044856921887\n",
      "Adjusted Precision:\n",
      "On the training data, Adjusted Downtrend precision:  -0.14634487583071007 Adjusted Uptrend precision:  0.15137129887905193\n",
      "On the validation data, Adjusted Downtrend precision:  -0.13932490623697735 Adjusted Uptrend precision:  0.15380876328032775\n",
      "On the testing data, Adjusted Downtrend precision:  -0.09085009733939003 Adjusted Uptrend precision:  0.15208043310131475\n",
      "**************************I'm the Divider*****************************\n",
      "The 2 th out of  100 Epochs in total has finished and loss in this epoch is 17172195.864257812\n",
      "The classification loss: training:  3033.5808  validation:  2972.7769  test:  1877.2101\n",
      "The regression loss: training:  12081268.0  validation:  11888044.0  test:  12138270.0\n",
      "The fluctuation loss: training:  19029232.0  validation:  18717780.0  test:  19178522.0\n",
      "The classification accuracy, train:  0.37863892  validation:  0.3727787  test:  0.37870318\n",
      "The train regret of the current epoch is:  0.7311199050913827\n",
      "The test regret of the current epoch is:  0.7225569556872236\n",
      "Recall Info:\n",
      "On the training data, the recall of Non-trend: 0.4001590525015266  Downtrend: 0.21629282721222579  Uptrend: 0.40620205358196626\n",
      "On the validation data, the recall of Non-trend: 0.39989175947816674  Downtrend: 0.22007195137079766  Uptrend: 0.40570726858078476\n",
      "On the testing data, the recall of Non-trend: 0.37280752365566583  Downtrend: 0.17819927408968506  Uptrend: 0.49628964691801314\n",
      "Precision Info:\n",
      "On the training data, the precision of Non-trend: 0.5873353343338336  Downtrend: 0.13905395768956502  Uptrend: 0.2909402142902428\n",
      "On the validation data, the precision of Non-trend: 0.5840336134453782  Downtrend: 0.1406262386048355  Uptrend: 0.2937838753387534\n",
      "On the testing data, the precision of Non-trend: 0.5809656536594138  Downtrend: 0.1626068376068376  Uptrend: 0.2929250114796369\n",
      "Adjusted Precision:\n",
      "On the training data, Adjusted Downtrend precision:  -0.12764440218683148 Adjusted Uptrend precision:  0.15752657494875208\n",
      "On the validation data, Adjusted Downtrend precision:  -0.12286959968291719 Adjusted Uptrend precision:  0.16137364498644988\n",
      "On the testing data, Adjusted Downtrend precision:  -0.08803418803418803 Adjusted Uptrend precision:  0.15979654551234504\n",
      "**************************I'm the Divider*****************************\n",
      "The 3 th out of  100 Epochs in total has finished and loss in this epoch is 4261432.227661133\n",
      "The classification loss: training:  1173.2491  validation:  1141.313  test:  732.72046\n",
      "The regression loss: training:  1958708.4  validation:  1975756.5  test:  2086499.8\n",
      "The fluctuation loss: training:  3762816.8  validation:  3712210.2  test:  3790297.5\n",
      "The classification accuracy, train:  0.38292807  validation:  0.37740797  test:  0.38960195\n",
      "The train regret of the current epoch is:  0.7195466952056215\n",
      "The test regret of the current epoch is:  0.7075690561628974\n",
      "Recall Info:\n",
      "On the training data, the recall of Non-trend: 0.3994631978073477  Downtrend: 0.2277544983978309  Uptrend: 0.42163342381682994\n",
      "On the validation data, the recall of Non-trend: 0.39402398382089043  Downtrend: 0.22466195261133853  Uptrend: 0.41757791941991695\n",
      "On the testing data, the recall of Non-trend: 0.4214458804523425  Downtrend: 0.1643835616438356  Uptrend: 0.43800119688809097\n",
      "Precision Info:\n",
      "On the training data, the precision of Non-trend: 0.586595207807645  Downtrend: 0.1485709691683081  Uptrend: 0.2995367556123839\n",
      "On the validation data, the precision of Non-trend: 0.5795868772782503  Downtrend: 0.14361617763679618  Uptrend: 0.3001429051782112\n",
      "On the testing data, the precision of Non-trend: 0.5768380320619126  Downtrend: 0.16363636363636364  Uptrend: 0.28140259141066554\n",
      "Adjusted Precision:\n",
      "On the training data, Adjusted Downtrend precision:  -0.1009366081119106 Adjusted Uptrend precision:  0.17161003626302218\n",
      "On the validation data, Adjusted Downtrend precision:  -0.10388580491673277 Adjusted Uptrend precision:  0.17173839946200406\n",
      "On the testing data, Adjusted Downtrend precision:  -0.0900932400932401 Adjusted Uptrend precision:  0.14168172555653813\n",
      "**************************I'm the Divider*****************************\n",
      "The 4 th out of  100 Epochs in total has finished and loss in this epoch is 866449.6586151123\n",
      "The classification loss: training:  412.19217  validation:  421.343  test:  241.24406\n",
      "The regression loss: training:  349564.75  validation:  344079.94  test:  353759.06\n",
      "The fluctuation loss: training:  659157.3  validation:  618948.1  test:  665203.4\n",
      "The classification accuracy, train:  0.3686835  validation:  0.36431664  test:  0.41697404\n",
      "The train regret of the current epoch is:  0.7472394368534143\n",
      "The test regret of the current epoch is:  0.6684970374697488\n",
      "Recall Info:\n",
      "On the training data, the recall of Non-trend: 0.4478606018433049  Downtrend: 0.2766206556568893  Uptrend: 0.2466658798536528\n",
      "On the validation data, the recall of Non-trend: 0.43959894038225994  Downtrend: 0.283711698300459  Uptrend: 0.2446640547336413\n",
      "On the testing data, the recall of Non-trend: 0.535685437341334  Downtrend: 0.2084065097763728  Uptrend: 0.2799521244763615\n",
      "Precision Info:\n",
      "On the training data, the precision of Non-trend: 0.5745595656688954  Downtrend: 0.12779343524923847  Uptrend: 0.2739097670456407\n",
      "On the validation data, the precision of Non-trend: 0.565477062875568  Downtrend: 0.13001705514496872  Uptrend: 0.2719178527328264\n",
      "On the testing data, the precision of Non-trend: 0.5767307513122341  Downtrend: 0.16527390900649955  Uptrend: 0.27602076941232\n",
      "Adjusted Precision:\n",
      "On the training data, Adjusted Downtrend precision:  -0.14846129757736215 Adjusted Uptrend precision:  0.13580813210576323\n",
      "On the validation data, Adjusted Downtrend precision:  -0.1450255827174531 Adjusted Uptrend precision:  0.14232793916942876\n",
      "On the testing data, Adjusted Downtrend precision:  -0.0756731662024141 Adjusted Uptrend precision:  0.12438045787113522\n",
      "**************************I'm the Divider*****************************\n",
      "The 5 th out of  100 Epochs in total has finished and loss in this epoch is 252782.1036605835\n",
      "The classification loss: training:  192.53172  validation:  191.44562  test:  126.418816\n",
      "The regression loss: training:  76934.54  validation:  78625.25  test:  75192.77\n",
      "The fluctuation loss: training:  153571.67  validation:  143634.64  test:  114286.11\n",
      "The classification accuracy, train:  0.3535429  validation:  0.3534985  test:  0.34135026\n",
      "The train regret of the current epoch is:  0.761956909496669\n",
      "The test regret of the current epoch is:  0.7815572060418927\n",
      "Recall Info:\n",
      "On the training data, the recall of Non-trend: 0.34375221892440744  Downtrend: 0.19805274833620903  Uptrend: 0.45243715331051576\n",
      "On the validation data, the recall of Non-trend: 0.34221095508018345  Downtrend: 0.19836248604391515  Uptrend: 0.4461142623238407\n",
      "On the testing data, the recall of Non-trend: 0.3079852296330487  Downtrend: 0.16098817468680482  Uptrend: 0.5065828845002992\n",
      "Precision Info:\n",
      "On the training data, the precision of Non-trend: 0.580243066375818  Downtrend: 0.13045949017697678  Uptrend: 0.28299867119444855\n",
      "On the validation data, the precision of Non-trend: 0.575796788880901  Downtrend: 0.12936893203883496  Uptrend: 0.2820958438100873\n",
      "On the testing data, the precision of Non-trend: 0.5721635671793772  Downtrend: 0.12371783336332554  Uptrend: 0.280837369783027\n",
      "Adjusted Precision:\n",
      "On the training data, Adjusted Downtrend precision:  -0.14247442766682905 Adjusted Uptrend precision:  0.14758969437472316\n",
      "On the validation data, Adjusted Downtrend precision:  -0.1487864077669903 Adjusted Uptrend precision:  0.14731548587487062\n",
      "On the testing data, Adjusted Downtrend precision:  -0.1588986863415512 Adjusted Uptrend precision:  0.13861057660407403\n",
      "**************************I'm the Divider*****************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 6 th out of  100 Epochs in total has finished and loss in this epoch is 119775.55953216553\n",
      "The classification loss: training:  148.59787  validation:  149.88722  test:  109.70625\n",
      "The regression loss: training:  26542.445  validation:  27056.064  test:  18540.139\n",
      "The fluctuation loss: training:  45397.266  validation:  39238.973  test:  35953.27\n",
      "The classification accuracy, train:  0.45794237  validation:  0.4553087  test:  0.4546107\n",
      "The train regret of the current epoch is:  0.6141350788554552\n",
      "The test regret of the current epoch is:  0.6128515396812151\n",
      "Recall Info:\n",
      "On the training data, the recall of Non-trend: 0.675277276793956  Downtrend: 0.19965491742666996  Uptrend: 0.12858491679452377\n",
      "On the validation data, the recall of Non-trend: 0.6749081379781809  Downtrend: 0.19488897159161395  Uptrend: 0.12274136015437694\n",
      "On the testing data, the recall of Non-trend: 0.6769559196861297  Downtrend: 0.17726261561878  Uptrend: 0.136983842010772\n",
      "Precision Info:\n",
      "On the training data, the precision of Non-trend: 0.583913550684595  Downtrend: 0.1366800253111158  Uptrend: 0.2830421510683899\n",
      "On the validation data, the precision of Non-trend: 0.5800386790374306  Downtrend: 0.13318073923363852  Uptrend: 0.2753147953830011\n",
      "On the testing data, the precision of Non-trend: 0.5745696726328934  Downtrend: 0.15161225716002402  Uptrend: 0.2518705985915493\n",
      "Adjusted Precision:\n",
      "On the training data, Adjusted Downtrend precision:  -0.13921113689095124 Adjusted Uptrend precision:  0.14678184061830224\n",
      "On the validation data, Adjusted Downtrend precision:  -0.14852492370295015 Adjusted Uptrend precision:  0.14231374606505773\n",
      "On the testing data, Adjusted Downtrend precision:  -0.11035449629481275 Adjusted Uptrend precision:  0.0915492957746479\n",
      "**************************I'm the Divider*****************************\n",
      "The 7 th out of  100 Epochs in total has finished and loss in this epoch is 85167.30589294434\n",
      "The classification loss: training:  111.26752  validation:  111.87519  test:  79.48887\n",
      "The regression loss: training:  8763.812  validation:  4836.1865  test:  10711.411\n",
      "The fluctuation loss: training:  20782.693  validation:  18025.102  test:  24789.193\n",
      "The classification accuracy, train:  0.35102084  validation:  0.35334915  test:  0.34215137\n",
      "The train regret of the current epoch is:  0.7824651351867061\n",
      "The test regret of the current epoch is:  0.7932237336226321\n",
      "Recall Info:\n",
      "On the training data, the recall of Non-trend: 0.3937401479756309  Downtrend: 0.29356667488291843  Uptrend: 0.28835713442700345\n",
      "On the validation data, the recall of Non-trend: 0.39388156208163616  Downtrend: 0.3038084604887731  Uptrend: 0.29284837144026665\n",
      "On the testing data, the recall of Non-trend: 0.34825755827371335  Downtrend: 0.3092143777075284  Uptrend: 0.3472172351885099\n",
      "Precision Info:\n",
      "On the training data, the precision of Non-trend: 0.5865826052002454  Downtrend: 0.12097511427120365  Uptrend: 0.28837415166715846\n",
      "On the validation data, the precision of Non-trend: 0.5879751679564589  Downtrend: 0.12422643806431977  Uptrend: 0.29394846510535894\n",
      "On the testing data, the precision of Non-trend: 0.5816710031801099  Downtrend: 0.14173777706220148  Uptrend: 0.28263834762275913\n",
      "Adjusted Precision:\n",
      "On the training data, Adjusted Downtrend precision:  -0.16079228034535298 Adjusted Uptrend precision:  0.14243139569194455\n",
      "On the validation data, Adjusted Downtrend precision:  -0.15861824084407022 Adjusted Uptrend precision:  0.1507894582379527\n",
      "On the testing data, Adjusted Downtrend precision:  -0.13341920249020556 Adjusted Uptrend precision:  0.13654520654715507\n",
      "**************************I'm the Divider*****************************\n",
      "The 8 th out of  100 Epochs in total has finished and loss in this epoch is 79979.01661682129\n",
      "The classification loss: training:  120.051094  validation:  118.13934  test:  85.99167\n",
      "The regression loss: training:  4240.498  validation:  4429.039  test:  5893.6646\n",
      "The fluctuation loss: training:  16535.822  validation:  12512.806  test:  16376.923\n",
      "The classification accuracy, train:  0.32813162  validation:  0.332642  test:  0.33041808\n",
      "The train regret of the current epoch is:  0.8023096642524702\n",
      "The test regret of the current epoch is:  0.8000834515563715\n",
      "Recall Info:\n",
      "On the training data, the recall of Non-trend: 0.23382137836033912  Downtrend: 0.2066798126694602  Uptrend: 0.5793992682638971\n",
      "On the validation data, the recall of Non-trend: 0.2381861167288575  Downtrend: 0.2126287061158665  Uptrend: 0.5851119817554529\n",
      "On the testing data, the recall of Non-trend: 0.22931571659358413  Downtrend: 0.19447371502166022  Uptrend: 0.6074805505685218\n",
      "Precision Info:\n",
      "On the training data, the precision of Non-trend: 0.5887295741409518  Downtrend: 0.14011195588603895  Uptrend: 0.28612017717682714\n",
      "On the validation data, the precision of Non-trend: 0.5958386774975061  Downtrend: 0.14403361344537816  Uptrend: 0.29142274646861804\n",
      "On the testing data, the precision of Non-trend: 0.5842704887908857  Downtrend: 0.15255326965466567  Uptrend: 0.2865733160182937\n",
      "Adjusted Precision:\n",
      "On the training data, Adjusted Downtrend precision:  -0.13204946110786195 Adjusted Uptrend precision:  0.15335412052686795\n",
      "On the validation data, Adjusted Downtrend precision:  -0.1287394957983193 Adjusted Uptrend precision:  0.1596621523227028\n",
      "On the testing data, Adjusted Downtrend precision:  -0.10626377663482736 Adjusted Uptrend precision:  0.14640618824459375\n",
      "**************************I'm the Divider*****************************\n",
      "The 9 th out of  100 Epochs in total has finished and loss in this epoch is 79337.075340271\n",
      "The classification loss: training:  110.015526  validation:  108.88667  test:  84.459465\n",
      "The regression loss: training:  3898.7239  validation:  4112.8037  test:  2165.3682\n",
      "The fluctuation loss: training:  6271.4233  validation:  6710.0254  test:  6260.2163\n",
      "The classification accuracy, train:  0.42111552  validation:  0.42453003  test:  0.4101978\n",
      "The train regret of the current epoch is:  0.6649244630279499\n",
      "The test regret of the current epoch is:  0.6828006342318285\n",
      "Recall Info:\n",
      "On the training data, the recall of Non-trend: 0.4942414473777639  Downtrend: 0.10247719990140498  Uptrend: 0.42107281954443526\n",
      "On the validation data, the recall of Non-trend: 0.49725126043239243  Downtrend: 0.10929165115990572  Uptrend: 0.4274603824337758\n",
      "On the testing data, the recall of Non-trend: 0.4653819524578814  Downtrend: 0.09834913944502986  Uptrend: 0.4543387193297427\n",
      "Precision Info:\n",
      "On the training data, the precision of Non-trend: 0.5934926075612627  Downtrend: 0.13480869001297016  Uptrend: 0.287953995157385\n",
      "On the validation data, the precision of Non-trend: 0.5946452294171748  Downtrend: 0.14132178376644208  Uptrend: 0.2962152524515763\n",
      "On the testing data, the precision of Non-trend: 0.5889310747663551  Downtrend: 0.1437125748502994  Uptrend: 0.2845790539020916\n",
      "Adjusted Precision:\n",
      "On the training data, Adjusted Downtrend precision:  -0.13951037613488979 Adjusted Uptrend precision:  0.14743744955609364\n",
      "On the validation data, Adjusted Downtrend precision:  -0.1267244145011229 Adjusted Uptrend precision:  0.15844071642758734\n",
      "On the testing data, Adjusted Downtrend precision:  -0.13122326775021384 Adjusted Uptrend precision:  0.13644201214483842\n",
      "**************************I'm the Divider*****************************\n",
      "The 10 th out of  100 Epochs in total has finished and loss in this epoch is 81137.70603942871\n",
      "The classification loss: training:  152.00119  validation:  150.84108  test:  128.74199\n",
      "The regression loss: training:  3461.9631  validation:  1683.9507  test:  3306.461\n",
      "The fluctuation loss: training:  4608.4185  validation:  4830.759  test:  7409.647\n",
      "The classification accuracy, train:  0.29524544  validation:  0.29831257  test:  0.29331553\n",
      "The train regret of the current epoch is:  0.8408870305383409\n",
      "The test regret of the current epoch is:  0.8504214303596762\n",
      "Recall Info:\n",
      "On the training data, the recall of Non-trend: 0.09168240623713024  Downtrend: 0.11098102045846685  Uptrend: 0.8057653723592588\n",
      "On the validation data, the recall of Non-trend: 0.09419773834278064  Downtrend: 0.10904354298474135  Uptrend: 0.8062686392608619\n",
      "On the testing data, the recall of Non-trend: 0.0795638126009693  Downtrend: 0.09062170706006323  Uptrend: 0.8366247755834829\n",
      "Precision Info:\n",
      "On the training data, the precision of Non-trend: 0.5827240725697265  Downtrend: 0.13557663354411323  Uptrend: 0.2839540832241562\n",
      "On the validation data, the precision of Non-trend: 0.5945702984537936  Downtrend: 0.131331241595697  Uptrend: 0.28716624317907274\n",
      "On the testing data, the precision of Non-trend: 0.5861849096705632  Downtrend: 0.14320074005550415  Uptrend: 0.28069470936652946\n",
      "Adjusted Precision:\n",
      "On the training data, Adjusted Downtrend precision:  -0.12541403191809694 Adjusted Uptrend precision:  0.14961424085511676\n",
      "On the validation data, Adjusted Downtrend precision:  -0.1386523233228746 Adjusted Uptrend precision:  0.15318448785770813\n",
      "On the testing data, Adjusted Downtrend precision:  -0.13135985198889916 Adjusted Uptrend precision:  0.13882140347354682\n",
      "**************************I'm the Divider*****************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 11 th out of  100 Epochs in total has finished and loss in this epoch is 72015.8387298584\n",
      "The classification loss: training:  105.5665  validation:  104.66159  test:  80.88751\n",
      "The regression loss: training:  2420.5068  validation:  1499.9949  test:  1070.9803\n",
      "The fluctuation loss: training:  3749.9524  validation:  2078.1064  test:  1812.0166\n",
      "The classification accuracy, train:  0.4561172  validation:  0.4567854  test:  0.45022115\n",
      "The train regret of the current epoch is:  0.6095057949011506\n",
      "The test regret of the current epoch is:  0.6187766001835934\n",
      "Recall Info:\n",
      "On the training data, the recall of Non-trend: 0.5828564125140236  Downtrend: 0.054288883411387726  Uptrend: 0.38625634367992445\n",
      "On the validation data, the recall of Non-trend: 0.5823909761586008  Downtrend: 0.055576231236819255  Uptrend: 0.38582539032805097\n",
      "On the testing data, the recall of Non-trend: 0.5737075928917609  Downtrend: 0.048354993560473014  Uptrend: 0.3964093357271095\n",
      "Precision Info:\n",
      "On the training data, the precision of Non-trend: 0.5927214961369052  Downtrend: 0.13832626785994662  Uptrend: 0.29140974556463284\n",
      "On the validation data, the precision of Non-trend: 0.5899870148607704  Downtrend: 0.1383142945353504  Uptrend: 0.2948826815642458\n",
      "On the testing data, the precision of Non-trend: 0.5846022693867953  Downtrend: 0.17979973878972572  Uptrend: 0.28067796610169493\n",
      "Adjusted Precision:\n",
      "On the training data, Adjusted Downtrend precision:  -0.12388130004710318 Adjusted Uptrend precision:  0.15170402689045703\n",
      "On the validation data, Adjusted Downtrend precision:  -0.13893176906452612 Adjusted Uptrend precision:  0.1604022346368715\n",
      "On the testing data, Adjusted Downtrend precision:  -0.04875925119721378 Adjusted Uptrend precision:  0.12991525423728814\n",
      "**************************I'm the Divider*****************************\n",
      "The 12 th out of  100 Epochs in total has finished and loss in this epoch is 76429.83912658691\n",
      "The classification loss: training:  141.14867  validation:  141.75842  test:  120.15404\n",
      "The regression loss: training:  1574.2753  validation:  1109.4521  test:  3518.3008\n",
      "The fluctuation loss: training:  2947.2185  validation:  960.293  test:  1813.8286\n",
      "The classification accuracy, train:  0.47217867  validation:  0.47321177  test:  0.46742886\n",
      "The train regret of the current epoch is:  0.5833478516969893\n",
      "The test regret of the current epoch is:  0.5922223149461737\n",
      "Recall Info:\n",
      "On the training data, the recall of Non-trend: 0.666259568002045  Downtrend: 0.08497658368252403  Uptrend: 0.2575534049333176\n",
      "On the validation data, the recall of Non-trend: 0.6681288631896772  Downtrend: 0.08795434809576975  Uptrend: 0.252909186597275\n",
      "On the testing data, the recall of Non-trend: 0.6716766674359567  Downtrend: 0.08383093314600164  Uptrend: 0.23776181926989826\n",
      "Precision Info:\n",
      "On the training data, the precision of Non-trend: 0.5855277937248833  Downtrend: 0.1297760210803689  Uptrend: 0.2930669800235018\n",
      "On the validation data, the precision of Non-trend: 0.5829025844930418  Downtrend: 0.13590185930611462  Uptrend: 0.29199297866594653\n",
      "On the testing data, the precision of Non-trend: 0.5776702642352065  Downtrend: 0.13857170505128702  Uptrend: 0.2750813542892751\n",
      "Adjusted Precision:\n",
      "On the training data, Adjusted Downtrend precision:  -0.13768115942028986 Adjusted Uptrend precision:  0.16011415141849927\n",
      "On the validation data, Adjusted Downtrend precision:  -0.12420931569867741 Adjusted Uptrend precision:  0.15784499054820417\n",
      "On the testing data, Adjusted Downtrend precision:  -0.13528159473582346 Adjusted Uptrend precision:  0.12795125666412796\n",
      "**************************I'm the Divider*****************************\n",
      "Network converged!\n"
     ]
    }
   ],
   "source": [
    "train_epoch_wise_info, valid_epoch_wise_info, test_epoch_wise_info = train_recurrent_neural_network(x_time_series, x_SP_data, x_time_series_prev, x_SP_data_prev, x_current_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = '../results_stock_split/'\n",
    "# make the path if it does not exist yet\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "# save training\n",
    "np.save(save_path+'train_precision_epoch_array.npy',train_epoch_wise_info[0])\n",
    "np.save(save_path+'train_recall_epoch_array.npy',train_epoch_wise_info[1]) \n",
    "np.save(save_path+'train_regret_epoch_array.npy',train_epoch_wise_info[2]) \n",
    "np.save(save_path+'save_class_matrix_train.npy',train_epoch_wise_info[3]) \n",
    "# save validation\n",
    "np.save(save_path+'valid_precision_epoch_array.npy',valid_epoch_wise_info[0])\n",
    "np.save(save_path+'valid_recall_epoch_array.npy',valid_epoch_wise_info[1]) \n",
    "np.save(save_path+'valid_regret_epoch_array.npy',valid_epoch_wise_info[2]) \n",
    "np.save(save_path+'save_class_matrix_valid.npy',valid_epoch_wise_info[3]) \n",
    "# save the testing\n",
    "np.save(save_path+'test_precision_epoch_array.npy',test_epoch_wise_info[0])\n",
    "np.save(save_path+'test_recall_epoch_array.npy',test_epoch_wise_info[1]) \n",
    "np.save(save_path+'test_regret_epoch_array.npy',test_epoch_wise_info[2]) \n",
    "np.save(save_path+'save_class_matrix_test.npy',test_epoch_wise_info[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}