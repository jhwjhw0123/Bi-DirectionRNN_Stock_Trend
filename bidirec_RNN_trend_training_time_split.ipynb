{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\iroooon\\anaconda3\\envs\\sci_computing\\lib\\importlib\\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "c:\\users\\iroooon\\anaconda3\\envs\\sci_computing\\lib\\importlib\\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from Metric_Computation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify the path to read\n",
    "data_load_path = '../data/intermediate_time_split/'\n",
    "# read the data\n",
    "# train\n",
    "data_train_X = np.load(data_load_path+'data_X_train.npy')\n",
    "data_train_X_prev = np.load(data_load_path+'data_X_prev_train.npy')\n",
    "SP500_train_X = np.load(data_load_path+'SP500_train.npy')\n",
    "SP500_train_X_prev = np.load(data_load_path+'SP500_prev_train.npy')\n",
    "value_target_train = np.load(data_load_path+'target_value_train.npy')\n",
    "value_target_train_prev = np.load(data_load_path+'target_value_prev_train.npy')\n",
    "gradient_target_train = np.load(data_load_path+'target_gradient_train.npy')\n",
    "gradient_target_train_prev = np.load(data_load_path+'target_gradient_prev_train.npy')\n",
    "trend_target_train = np.load(data_load_path+'price_trend_flag_train.npy')\n",
    "# validation\n",
    "data_valid_X = np.load(data_load_path+'data_X_valid.npy')\n",
    "data_valid_X_prev = np.load(data_load_path+'data_X_prev_valid.npy')\n",
    "SP500_valid_X = np.load(data_load_path+'SP500_valid.npy')\n",
    "SP500_valid_X_prev = np.load(data_load_path+'SP500_prev_valid.npy')\n",
    "value_target_valid = np.load(data_load_path+'target_value_valid.npy')\n",
    "value_target_valid_prev = np.load(data_load_path+'target_value_prev_valid.npy')\n",
    "gradient_target_valid = np.load(data_load_path+'target_gradient_valid.npy')\n",
    "gradient_target_valid_prev = np.load(data_load_path+'target_gradient_prev_valid.npy')\n",
    "trend_target_valid = np.load(data_load_path+'price_trend_flag_valid.npy')\n",
    "# test\n",
    "data_test_X = np.load(data_load_path+'data_X_test.npy')\n",
    "data_test_X_prev = np.load(data_load_path+'data_X_prev_test.npy')\n",
    "SP500_test_X = np.load(data_load_path+'SP500_test.npy')\n",
    "SP500_test_X_prev = np.load(data_load_path+'SP500_prev_test.npy')\n",
    "value_target_test = np.load(data_load_path+'target_value_test.npy')\n",
    "value_target_test_prev = np.load(data_load_path+'target_value_prev_test.npy')\n",
    "gradient_target_test = np.load(data_load_path+'target_gradient_test.npy')\n",
    "gradient_target_test_prev = np.load(data_load_path+'target_gradient_prev_test.npy')\n",
    "trend_target_test = np.load(data_load_path+'price_trend_flag_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training set: drop instance that contains inf values\n",
    "valid_series_index_train = np.where((np.sum(np.sum(data_train_X,axis=-1),axis=-1)!=np.inf)&((np.sum(np.sum(data_train_X_prev,axis=-1),axis=-1)!=np.inf)))[0]\n",
    "data_train_X = data_train_X[valid_series_index_train]\n",
    "data_train_X_prev = data_train_X_prev[valid_series_index_train]\n",
    "SP500_train_X = np.reshape(SP500_train_X[valid_series_index_train],[-1,1])\n",
    "SP500_train_X_prev = np.reshape(SP500_train_X_prev[valid_series_index_train],[-1,1])\n",
    "value_target_train = np.reshape(value_target_train[valid_series_index_train],[-1,1])\n",
    "value_target_train_prev = np.reshape(value_target_train_prev[valid_series_index_train],[-1,1])\n",
    "gradient_target_train = np.reshape(gradient_target_train[valid_series_index_train],[-1,1])\n",
    "gradient_target_train_prev = np.reshape(gradient_target_train_prev[valid_series_index_train],[-1,1])\n",
    "trend_target_train = np.reshape(trend_target_train[valid_series_index_train],[-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validation set: drop instance that contains inf values\n",
    "valid_series_index_valid = np.where((np.sum(np.sum(data_valid_X,axis=-1),axis=-1)!=np.inf)&((np.sum(np.sum(data_valid_X_prev,axis=-1),axis=-1)!=np.inf)))[0]\n",
    "data_valid_X = data_valid_X[valid_series_index_valid]\n",
    "data_valid_X_prev = data_valid_X_prev[valid_series_index_valid]\n",
    "SP500_valid_X = np.reshape(SP500_valid_X[valid_series_index_valid],[-1,1])\n",
    "SP500_valid_X_prev = np.reshape(SP500_valid_X_prev[valid_series_index_valid],[-1,1])\n",
    "value_target_valid = np.reshape(value_target_valid[valid_series_index_valid],[-1,1])\n",
    "value_target_valid_prev = np.reshape(value_target_valid_prev[valid_series_index_valid],[-1,1])\n",
    "gradient_target_valid = np.reshape(gradient_target_valid[valid_series_index_valid],[-1,1])\n",
    "gradient_target_valid_prev = np.reshape(gradient_target_valid_prev[valid_series_index_valid],[-1,1])\n",
    "trend_target_valid = np.reshape(trend_target_valid[valid_series_index_valid],[-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing set: drop instance that contains inf values\n",
    "valid_series_index_test = np.where((np.sum(np.sum(data_test_X,axis=-1),axis=-1)!=np.inf)&((np.sum(np.sum(data_test_X_prev,axis=-1),axis=-1)!=np.inf)))[0]\n",
    "data_test_X = data_test_X[valid_series_index_test]\n",
    "data_test_X_prev = data_test_X_prev[valid_series_index_test]\n",
    "SP500_test_X = np.reshape(SP500_test_X[valid_series_index_test],[-1,1])\n",
    "SP500_test_X_prev = np.reshape(SP500_test_X_prev[valid_series_index_test],[-1,1])\n",
    "value_target_test = np.reshape(value_target_test[valid_series_index_test],[-1,1])\n",
    "value_target_test_prev = np.reshape(value_target_test_prev[valid_series_index_test],[-1,1])\n",
    "gradient_target_test = np.reshape(gradient_target_test[valid_series_index_test],[-1,1])\n",
    "gradient_target_test_prev = np.reshape(gradient_target_test_prev[valid_series_index_test],[-1,1])\n",
    "trend_target_test = np.reshape(trend_target_test[valid_series_index_test],[-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_and_future_price_train = np.concatenate([value_target_train_prev, value_target_train],axis=1)\n",
    "current_and_future_price_valid = np.concatenate([value_target_valid_prev, value_target_valid],axis=1)\n",
    "current_and_future_price_test = np.concatenate([value_target_test_prev, value_target_test],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preceed the trend(class) target to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding_label(label_input, num_class=None):\n",
    "    '''\n",
    "    :param label_input: The sparse form of input label (2,0,1,3,0,1,2etc.)\n",
    "    :param num_class: The number of classes, if keep None, then automatically infer from the given label input\n",
    "    '''\n",
    "    # retrieve the number of input data\n",
    "    nData = label_input.shape[0]\n",
    "    # reshape the data\n",
    "    label_input_flat = np.reshape(label_input, [-1])\n",
    "    if (label_input_flat.shape[0]!=nData):            # which means the input label is not 'mathematically 1-d'\n",
    "        raise ValueError('The input label must be 1-d mathematically')\n",
    "    # infer the number of class if input is None\n",
    "    if num_class is None:\n",
    "        num_class = (int)(np.amax(label_input)+1)\n",
    "    # create the return encoded matrx\n",
    "    one_hot_label_mat = np.zeros([nData, num_class])\n",
    "    # get a row index to assist the batch-assigning\n",
    "    row_ind_batch = np.arange(nData)\n",
    "    # assign '1's to the corresponding positions\n",
    "    one_hot_label_mat[row_ind_batch, label_input_flat.astype('int')] = 1\n",
    "    \n",
    "    return one_hot_label_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trend_target_train_input = one_hot_encoding_label(trend_target_train)\n",
    "trend_target_valid_input = one_hot_encoding_label(trend_target_valid)\n",
    "trend_target_test_input = one_hot_encoding_label(trend_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# confusion metrics\n",
    "def classification_info_computation(pred_label, true_label, num_class):\n",
    "    '''\n",
    "    :param pred_label: the sparse (not one-hot) prediction of labels\n",
    "    :param true_label: the sparse (not one-hot) ground-truth of labels\n",
    "    :param num_class: number of classes\n",
    "    '''\n",
    "    # flatten the two label arrays if they are not already so\n",
    "    pred_label = np.reshape(pred_label,[-1])\n",
    "    true_label = np.reshape(true_label,[-1])\n",
    "    # initialize the confusion maxtrix array\n",
    "    class_matrix = np.zeros([num_class, num_class])    # each row is the true labels\n",
    "    # initialize the precision and recall arrays\n",
    "    precision_array = np.zeros([num_class])\n",
    "    recall_array = np.zeros([num_class])\n",
    "    # fill the confusion-prediction matrix\n",
    "    for cClass_True in range(num_class):\n",
    "        # retrieve the current \n",
    "        current_cClass_ind = np.where(true_label==cClass_True)[0]\n",
    "        # retrueve the corresponding predictions\n",
    "        current_cClass_pred = pred_label[current_cClass_ind]\n",
    "        # fill the evaluation matrx\n",
    "        for cClass_Pred in range(num_class):\n",
    "            cClass_pred_num = np.where(current_cClass_pred==cClass_Pred)[0].shape[0]\n",
    "            class_matrix[cClass_True, cClass_Pred] = cClass_pred_num\n",
    "    # fill the precision and recall arrays\n",
    "    for cClass_True in range(num_class):\n",
    "        precision_array[cClass_True] = class_matrix[cClass_True,cClass_True]/np.sum(class_matrix[:,cClass_True])\n",
    "        recall_array[cClass_True] = class_matrix[cClass_True,cClass_True]/np.sum(class_matrix[cClass_True,:])\n",
    "        \n",
    "    return class_matrix, precision_array, recall_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'regret' function\n",
    "def invest_regret_comput(pred_label, true_label):\n",
    "    '''\n",
    "    The function to return the 'regret' defined by the real investment scenarios\n",
    "    :param pred_label: the sparse (not one-hot) prediction of labels\n",
    "    :param true_label: the sparse (not one-hot) ground-truth of labels\n",
    "    With the meaning 2=uptrend 1=downtrend 0=non-trend\n",
    "    Strategy: \n",
    "        predict 0: don't buy or sell\n",
    "        predict 1: sell\n",
    "        predict 2: buy\n",
    "    ******************** Truth Table *********************\n",
    "    | True Label | Predicted Label | Regret |\n",
    "    |      0     |        0        |   0    |\n",
    "    |      0     |        1        |   1    |\n",
    "    |      0     |        2        |   1    |\n",
    "    |      1     |        0        |   1    |\n",
    "    |      1     |        1        |   0    |\n",
    "    |      1     |        2        |   2    |\n",
    "    |      2     |        0        |   1    |\n",
    "    |      2     |        1        |   2    |\n",
    "    |      2     |        2        |   0    |\n",
    "    '''\n",
    "    # flatten the two label arrays if they are not already so\n",
    "    pred_label = np.reshape(pred_label,[-1])\n",
    "    true_label = np.reshape(true_label,[-1])\n",
    "    # check if the two arrays are of the same legth\n",
    "    if pred_label.shape[0]!=true_label.shape[0]:\n",
    "        raise ValueError('The predicted and the true labels must be in the same length!')\n",
    "    # placeholder of regret array\n",
    "    regret_array = np.zeros([pred_label.shape[0]])\n",
    "    # check the conditions for regret '1'\n",
    "    one_regret_ind = np.where(((true_label==0)&(pred_label==2))|((true_label==0)&(pred_label==1))|((true_label==2)&(pred_label==0))|((true_label==1)&(pred_label==0)))[0]\n",
    "    # check the conditions for regret '2'\n",
    "    two_regret_ind = np.where(((true_label==1)&(pred_label==2))|((true_label==2)&(pred_label==1)))[0]\n",
    "    # assign regret values to the entries\n",
    "    regret_array[one_regret_ind] = 1.0\n",
    "    regret_array[two_regret_ind] = 2.0\n",
    "    # compute the overall regret\n",
    "    overall_regret = np.mean(regret_array)\n",
    "    \n",
    "    return overall_regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters of the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rnn cell sequence length and dimensions\n",
    "nChunck = data_train_X.shape[1]\n",
    "nDim = data_train_X.shape[2]\n",
    "# output dimension\n",
    "nClass = 3\n",
    "# number of nested RNN stack\n",
    "nStack_rnn = 5\n",
    "# rnn dimension\n",
    "rnn_size = 64\n",
    "# training hyper-parameter\n",
    "nEpochs = 100\n",
    "batch_size = 256\n",
    "pred_loss_coeff = 1e-3\n",
    "l2_FC_pen_coeff = 1e-3\n",
    "fluc_pen_coeff = 1e-4\n",
    "learning_rate = 1e-4\n",
    "tol = 1e-3\n",
    "focal_gamma = 3.0     # parameter for the focal loss\n",
    "trend_weight_alpha = 2.0  # parameter to encourage output 1/2\n",
    "# data info for feeding them\n",
    "nData_train = data_train_X.shape[0]\n",
    "nData_test = data_test_X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A 'Erease Graph' here to enable us to re-run things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Model_path = '../model/trend_bidirec_RNN_time_split/'\n",
    "#Model saving function\n",
    "def tf_save_model(session):\n",
    "    if not os.path.exists(Model_path):\n",
    "        os.makedirs(Model_path)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(session, Model_path+'Bidirec_RNN.checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the RNN graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the x input as float and reshape input\n",
    "x_time_series = tf.placeholder(\"float\",[None,nChunck,nDim])       # Batch_size * n_chunk * chunk_size\n",
    "x_time_series_prev = tf.placeholder(\"float\",[None,nChunck,nDim])  # Batch_size * n_chunk * chunk_size\n",
    "x_SP_data = tf.placeholder(\"float\",[None,1])                      # Batch_size * 1\n",
    "x_SP_data_prev = tf.placeholder(\"float\",[None,1])                 # Batch_size * 1\n",
    "x_current_price = tf.placeholder(\"float\",[None,2])                # Batch_size * 2\n",
    "y = tf.placeholder(\"float\",[None,1])                              # Batch_size * 1\n",
    "y_prev = tf.placeholder(\"float\",[None,1])                         # Batch_size * 1\n",
    "y_trend = tf.placeholder(\"float\",[None,nClass])                   # Batch_size * n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def swish_activation(data_input):\n",
    "    '''\n",
    "    Swish activation, by Ramachandran et al., Google Brain, 2017\n",
    "    The activation is found by a RNN-based combinational search, and it has been proved consistently outperforming\n",
    "        RELU on networks like NASNet-A and Inception-ResNet-v2\n",
    "    '''\n",
    "    # define the beta variable as trainable\n",
    "    para_beta = tf.Variable(tf.random_uniform([1]))\n",
    "    swish_output = data_input*tf.sigmoid(para_beta*data_input)\n",
    "    \n",
    "    return swish_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Recurrent_neural_network(x_time_series, x_SP_data, x_time_series_prev, x_SP_data_prev, x_current_price):\n",
    "    # process the data concatenating the current and previous data\n",
    "    data_processed = tf.concat([x_time_series,x_time_series_prev],axis=0)\n",
    "    # split the data into (nChunck * [batch_size, nDim])       \n",
    "    data_processed = tf.split(data_processed, num_or_size_splits=nChunck, axis=1)  # (nChunck * [batch_size, nDim])\n",
    "    data_processed = [tf.squeeze(this_time_data, axis=1) for this_time_data in data_processed]\n",
    "    # define individual GRU cells\n",
    "    # forward\n",
    "    stack_GRU_cell_forward = []\n",
    "    for _ in range(nStack_rnn):\n",
    "        current_rnn_cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.GRUCell(rnn_size),\n",
    "                                                         input_keep_prob=0.7,\n",
    "                                                         variational_recurrent=False)# ,\n",
    "#                                                          input_size=nDim,\n",
    "#                                                          dtype=tf.float32)\n",
    "        stack_GRU_cell_forward.append(current_rnn_cell)\n",
    "    heiarchy_RNN_forward = tf.contrib.rnn.MultiRNNCell(stack_GRU_cell_forward)\n",
    "    # backward\n",
    "    stack_GRU_cell_backward = []\n",
    "    for _ in range(nStack_rnn):\n",
    "        current_rnn_cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.GRUCell(rnn_size),\n",
    "                                                         input_keep_prob=0.7,\n",
    "                                                         variational_recurrent=False)#,\n",
    "#                                                          input_size=nDim,\n",
    "#                                                          dtype=tf.float32)\n",
    "        stack_GRU_cell_backward.append(current_rnn_cell)\n",
    "    heiarchy_RNN_backward = tf.contrib.rnn.MultiRNNCell(stack_GRU_cell_backward)\n",
    "    # output of the forward-backword RNN\n",
    "    outputs_all, output_for_final, output_back_final = tf.contrib.rnn.static_bidirectional_rnn(cell_fw=heiarchy_RNN_forward,\n",
    "                                                                                               cell_bw=heiarchy_RNN_backward,\n",
    "                                                                                               inputs=data_processed,\n",
    "                                                                                               dtype=tf.float32)\n",
    "    # Post-RNN process\n",
    "    # concatenate the current and prev S&P-500\n",
    "    x_SP_data_processed = tf.concat([x_SP_data,x_SP_data_prev],axis=0)\n",
    "    # [nBatch x 2*rnn_stack*rnn_size+1]\n",
    "    # print(outputs_all[-1].get_shape().as_list())\n",
    "    out_status_processed = tf.concat([outputs_all[-1], x_SP_data_processed],axis=1)   # [batch_size x (2*rnn_size+1)]\n",
    "    with tf.name_scope(\"FC_layers_rate\"):\n",
    "        weight_FC_rate = {'weight_hidden': tf.Variable(tf.random_normal([2*rnn_size+1, 64])),\n",
    "                     'weight_out': tf.Variable(tf.random_normal([64, 1]))}\n",
    "    with tf.name_scope(\"FC_layers_scale\"):\n",
    "        weight_FC_scale = {'weight_hidden': tf.Variable(tf.random_normal([2*rnn_size+1, 64])),\n",
    "                     'weight_out': tf.Variable(tf.random_normal([64, 1]))}\n",
    "    with tf.name_scope(\"FC_layers_class\"):\n",
    "        weight_FC_class = {'weight_hidden_1': tf.Variable(tf.random_normal([2*rnn_size+4, 128])),\n",
    "                           'weight_hidden_2': tf.Variable(tf.random_normal([128, 64])),\n",
    "                           'weight_out': tf.Variable(tf.random_normal([64, nClass]))}\n",
    "    # ***************rate prediction**************\n",
    "    # fully-connected hidden\n",
    "    process_output_rate = tf.matmul(out_status_processed, weight_FC_rate[\"weight_hidden\"])    # [batch_size x 64]\n",
    "    # non-linear activation \n",
    "    process_output_rate = swish_activation(process_output_rate)\n",
    "    process_output_rate = tf.nn.dropout(process_output_rate, keep_prob = 0.7)\n",
    "    #         process_output = tf.nn.tanh(process_output)\n",
    "    # process to the final output\n",
    "    final_output_rate = tf.nn.sigmoid(tf.matmul(process_output_rate, weight_FC_rate[\"weight_out\"]))  # [batch_size x 1]\n",
    "    # ***************scale prediction**************\n",
    "    process_output_scale = tf.matmul(out_status_processed, weight_FC_scale[\"weight_hidden\"])\n",
    "    # non-linear activation \n",
    "    process_output_scale = swish_activation(process_output_scale)\n",
    "    process_output_scale = tf.nn.dropout(process_output_scale, keep_prob = 0.7)\n",
    "    # out\n",
    "    # pass to RELU to limit the learning inside the regime of scaling\n",
    "    final_output_scale = tf.nn.relu(tf.matmul(process_output_scale, weight_FC_scale[\"weight_out\"]))\n",
    "    # combine them to get the final output\n",
    "    final_output = tf.multiply(final_output_rate, final_output_scale)\n",
    "    # split the results to get the prediction for current and previous data\n",
    "    current_prediction, prev_prediction = tf.split(final_output,num_or_size_splits=2,axis=0)\n",
    "    # ***************class prediction****************\n",
    "    out_status_processed_class = tf.concat([tf.split(out_status_processed,num_or_size_splits=2,axis=0)[0],  # only pick the 'current'\n",
    "                                            x_current_price,\n",
    "                                            current_prediction],axis=1)\n",
    "    # layer 1\n",
    "    process_output_class = tf.matmul(out_status_processed_class, weight_FC_class[\"weight_hidden_1\"])\n",
    "    process_output_class = swish_activation(process_output_class)\n",
    "    # layer 2\n",
    "    process_output_class = tf.matmul(process_output_class, weight_FC_class[\"weight_hidden_2\"])\n",
    "    process_output_class = swish_activation(process_output_class)\n",
    "    # layer 3 (out-layer)\n",
    "    final_output_class = tf.matmul(process_output_class, weight_FC_class[\"weight_out\"])            \n",
    "\n",
    "    return current_prediction, prev_prediction, final_output_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def focal_classification_loss(logits, labels, trend_weight):\n",
    "    '''\n",
    "    will be resemble the cross-entropy calssification loss, but with the focal loss-decrease term for well-classes \n",
    "    examples to tackle the problem of imbalance data.\n",
    "    :param logits: The [nData * nClass] un-normalised prediction\n",
    "    :param labels: The [nData * nClass] one-hot true labels\n",
    "    :param trend_weight: To encourage the network to predict up/down trends. Should be specify\n",
    "    :return: the loss of [nData] shape\n",
    "    ''' \n",
    "    # get the pobabilistic sotmax output\n",
    "    soft_max_prediction = tf.nn.softmax(logits, axis=1)\n",
    "    # process to log(soft_max) output, use this integrated function to faciliate numerical stability\n",
    "    log_soft_max_prediction = tf.nn.log_softmax(logits, axis=1)\n",
    "    # compute focal loss\n",
    "    class_weight = tf.constant([1.0, trend_weight, trend_weight])\n",
    "    # -sum(y*(1-p)^(gamma)*log(p))  \n",
    "    focal_loss = -tf.reduce_sum(tf.multiply(tf.multiply(labels,tf.multiply(tf.pow(1-soft_max_prediction,focal_gamma),\n",
    "                                                                           log_soft_max_prediction)),class_weight),\n",
    "                                axis=1)\n",
    "    \n",
    "    return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_recurrent_neural_network(x_time_series, x_SP_data, x_time_series_prev, x_SP_data_prev, x_current_price):\n",
    "    # get the prediction\n",
    "    grad_pred, grad_pred_prev, class_pred = Recurrent_neural_network(x_time_series, x_SP_data, x_time_series_prev, x_SP_data_prev, x_current_price)\n",
    "    # mean square loss\n",
    "    loss_prediction = tf.losses.mean_squared_error(predictions=grad_pred, labels=y)\n",
    "    # compute the loss of fluctuation\n",
    "    # use this scheme to back-prop gradient to both sides\n",
    "    loss_fluctuation = (tf.losses.mean_squared_error(predictions=grad_pred, labels=grad_pred_prev) + \n",
    "                        tf.losses.mean_squared_error(predictions=grad_pred_prev, labels=grad_pred))/2\n",
    "    # mis-classification loss\n",
    "    # loss_classification = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=class_pred, labels=y_trend))\n",
    "    # focal loss\n",
    "    loss_classification = tf.reduce_mean(focal_classification_loss(logits=class_pred, labels=y_trend, trend_weight=trend_weight_alpha))\n",
    "    # l_2 loss for FC layers\n",
    "    reg_vars = (tf.trainable_variables(scope='FC_layers_rate') \n",
    "                + tf.trainable_variables(scope='FC_layers_scale') \n",
    "                + tf.trainable_variables(scope='FC_layers_class'))\n",
    "    loss_l2_FC = tf.add_n([tf.nn.l2_loss(var) for var in reg_vars])\n",
    "    # overall loss\n",
    "    loss = loss_classification + pred_loss_coeff*loss_prediction + l2_FC_pen_coeff*loss_l2_FC + fluc_pen_coeff*loss_fluctuation\n",
    "    # defineing optimiser\n",
    "    optimiser_org = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    # apply gradient clipping to avoid gradient exploding\n",
    "    gradients_nn = optimiser_org.compute_gradients(loss)\n",
    "    clipped_grad = [(tf.clip_by_value(grad, -10., 10.), var) for grad, var in gradients_nn]\n",
    "    optimiser = optimiser_org.apply_gradients(clipped_grad)\n",
    "    prev_test_loss = 0\n",
    "    # evaluation tensor\n",
    "    label_flatten_tensor = tf.argmax(y_trend, 1)\n",
    "    logit_flatten_tensor = tf.argmax(class_pred,1)\n",
    "    acc_tensor = tf.reduce_mean(tf.cast(tf.equal(label_flatten_tensor, logit_flatten_tensor),tf.float32))\n",
    "    # the training dictionary\n",
    "    train_dict = {x_time_series: data_train_X,\n",
    "                  x_time_series_prev: data_train_X_prev,\n",
    "                  x_SP_data: SP500_train_X,\n",
    "                  x_SP_data_prev: SP500_train_X_prev,\n",
    "                  y: gradient_target_train,\n",
    "                  y_prev: gradient_target_train_prev, \n",
    "                  y_trend: trend_target_train_input,\n",
    "                  x_current_price: current_and_future_price_train}\n",
    "    # the validation dictionary\n",
    "    valid_dict = {x_time_series: data_valid_X,\n",
    "                 x_time_series_prev: data_valid_X_prev,\n",
    "                 x_SP_data: SP500_valid_X,\n",
    "                 x_SP_data_prev: SP500_valid_X_prev,\n",
    "                 y: gradient_target_valid,\n",
    "                 y_prev: gradient_target_valid_prev, \n",
    "                 y_trend: trend_target_valid_input, \n",
    "                 x_current_price: current_and_future_price_valid}\n",
    "    # the test dictionary\n",
    "    test_dict = {x_time_series: data_test_X,\n",
    "                 x_time_series_prev: data_test_X_prev,\n",
    "                 x_SP_data: SP500_test_X,\n",
    "                 x_SP_data_prev: SP500_test_X_prev,\n",
    "                 y: gradient_target_test,\n",
    "                 y_prev: gradient_target_test_prev, \n",
    "                 y_trend: trend_target_test_input, \n",
    "                 x_current_price: current_and_future_price_test}\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # define the lists to collect the infomation while training\n",
    "        # train\n",
    "        train_precision_epoch_collection = []\n",
    "        train_recall_epoch_collection = []\n",
    "        train_regret_epoch_collection = []\n",
    "        # valid\n",
    "        valid_precision_epoch_collection = []\n",
    "        valid_recall_epoch_collection = []\n",
    "        valid_regret_epoch_collection = []\n",
    "        # test\n",
    "        test_precision_epoch_collection = []\n",
    "        test_recall_epoch_collection = []\n",
    "        test_regret_epoch_collection = []\n",
    "        for cEpoch in range(nEpochs):\n",
    "            current_Epoch_Loss = 0\n",
    "            random_index = np.random.choice(nData_train, size=nData_train)    # Use this to avoid overflow for the derivative index\n",
    "            for i in range(nData_train//batch_size):\n",
    "                this_index = random_index[i*batch_size:(i+1)*batch_size]\n",
    "                current_x_time_series = data_train_X[this_index]\n",
    "                current_x_time_series_prev = data_train_X_prev[this_index]\n",
    "                current_x_SP_500 = SP500_train_X[this_index]\n",
    "                current_x_SP_500_prev = SP500_train_X_prev[this_index]\n",
    "                current_x_price_info = current_and_future_price_train[this_index]\n",
    "                current_y_gradient = gradient_target_train[this_index]\n",
    "                current_y_gradient_prev = gradient_target_train_prev[this_index]\n",
    "                current_y_trend = trend_target_train_input[this_index]\n",
    "                #Get how many batches we need for each epoch\n",
    "                this_feed_dict = {x_time_series: current_x_time_series,\n",
    "                                  x_time_series_prev: current_x_time_series_prev,\n",
    "                                  x_SP_data: current_x_SP_500, \n",
    "                                  x_SP_data_prev: current_x_SP_500_prev,\n",
    "                                  y: current_y_gradient, \n",
    "                                  y_prev: current_y_gradient_prev,\n",
    "                                  y_trend: current_y_trend, \n",
    "                                  x_current_price: current_x_price_info}\n",
    "                _, currentloss = sess.run([optimiser,loss], feed_dict = this_feed_dict)\n",
    "#                 print(currentloss)\n",
    "#                 print(np.amax(current_x_time_series))\n",
    "#                 print(np.amin(current_x_time_series))\n",
    "                current_Epoch_Loss += currentloss\n",
    "                # break\n",
    "            print('The',cEpoch+1,'th out of ',nEpochs,'Epochs in total has finished and loss in this epoch is',current_Epoch_Loss)\n",
    "            # Check the classification loss for training and testing data\n",
    "            train_data_loss = loss_classification.eval(feed_dict=train_dict)\n",
    "            valid_data_loss = loss_classification.eval(feed_dict=valid_dict)\n",
    "            test_data_loss = loss_classification.eval(feed_dict=test_dict)\n",
    "            print('The classification loss: training: ',train_data_loss, ' validation: ', valid_data_loss, ' test: ', test_data_loss)\n",
    "            # Check the regression loss for training and testing data\n",
    "            train_data_loss_reg = loss_prediction.eval(feed_dict=train_dict)\n",
    "            valid_data_loss_reg = loss_prediction.eval(feed_dict=valid_dict)\n",
    "            test_data_loss_reg = loss_prediction.eval(feed_dict=test_dict)\n",
    "            print('The regression loss: training: ',train_data_loss_reg, ' validation: ', valid_data_loss_reg, ' test: ', test_data_loss_reg)\n",
    "            # Check the fluctuation loss for training and testing data\n",
    "            train_data_loss_fluc = loss_fluctuation.eval(feed_dict=train_dict)\n",
    "            valid_data_loss_fluc = loss_fluctuation.eval(feed_dict=valid_dict)\n",
    "            test_data_loss_fluc = loss_fluctuation.eval(feed_dict=test_dict)\n",
    "            print('The fluctuation loss: training: ',train_data_loss_fluc, ' validation: ', valid_data_loss_fluc, ' test: ', test_data_loss_fluc)\n",
    "            # Also check the current classification accuracy\n",
    "            train_data_accuracy = acc_tensor.eval(feed_dict=train_dict)\n",
    "            valid_data_accuracy = acc_tensor.eval(feed_dict=valid_dict)\n",
    "            test_data_accuracy = acc_tensor.eval(feed_dict=test_dict)\n",
    "            # evluation and accuracy\n",
    "            # flattened predictions\n",
    "            # train\n",
    "            train_logit_flatten_numerical = logit_flatten_tensor.eval(train_dict)\n",
    "            train_label_flatten_numerical = label_flatten_tensor.eval(train_dict)\n",
    "            # validation\n",
    "            valid_logit_flatten_numerical = logit_flatten_tensor.eval(valid_dict)\n",
    "            valid_label_flatten_numerical = label_flatten_tensor.eval(valid_dict)\n",
    "            # test\n",
    "            test_logit_flatten_numerical = logit_flatten_tensor.eval(test_dict)\n",
    "            test_label_flatten_numerical = label_flatten_tensor.eval(test_dict)\n",
    "            # train\n",
    "            class_matrix_train, precision_array_train, recall_array_train = classification_info_computation(pred_label = train_logit_flatten_numerical, \n",
    "                                                                                                            true_label = train_label_flatten_numerical, \n",
    "                                                                                                            num_class=3)\n",
    "            down_mis_up_rate_train = class_matrix_train[2,1]/np.sum(class_matrix_train[:,1])\n",
    "            up_mis_down_rate_train = class_matrix_train[1,2]/np.sum(class_matrix_train[:,2])\n",
    "            precision_downtrend_adjusted_train = precision_array_train[1] - down_mis_up_rate_train\n",
    "            precision_uptrend_adjuest_train = precision_array_train[2] - up_mis_down_rate_train\n",
    "            # validation\n",
    "            class_matrix_valid, precision_array_valid, recall_array_valid = classification_info_computation(pred_label = valid_logit_flatten_numerical, \n",
    "                                                                                                            true_label = valid_label_flatten_numerical, \n",
    "                                                                                                            num_class=3)\n",
    "            down_mis_up_rate_valid = class_matrix_valid[2,1]/np.sum(class_matrix_valid[:,1])\n",
    "            up_mis_down_rate_valid = class_matrix_valid[1,2]/np.sum(class_matrix_valid[:,2])\n",
    "            precision_downtrend_adjusted_valid = precision_array_valid[1] - down_mis_up_rate_valid\n",
    "            precision_uptrend_adjuest_valid = precision_array_valid[2] - up_mis_down_rate_valid\n",
    "            # test\n",
    "            class_matrix_test, precision_array_test, recall_array_test = classification_info_computation(pred_label = test_logit_flatten_numerical, \n",
    "                                                                                                         true_label = test_label_flatten_numerical, \n",
    "                                                                                                         num_class=3)\n",
    "            down_mis_up_rate_test = class_matrix_test[2,1]/np.sum(class_matrix_test[:,1])\n",
    "            up_mis_down_rate_test = class_matrix_test[1,2]/np.sum(class_matrix_test[:,2])\n",
    "            precision_downtrend_adjusted_test = precision_array_test[1] - down_mis_up_rate_test\n",
    "            precision_uptrend_adjuest_test = precision_array_test[2] - up_mis_down_rate_test\n",
    "            # training and testing regret\n",
    "            regret_epoch_train = invest_regret_comput(pred_label = train_logit_flatten_numerical,\n",
    "                                                      true_label = train_label_flatten_numerical)\n",
    "            regret_epoch_valid = invest_regret_comput(pred_label = valid_logit_flatten_numerical,\n",
    "                                                      true_label = valid_label_flatten_numerical)\n",
    "            regret_epoch_test = invest_regret_comput(pred_label = test_logit_flatten_numerical,\n",
    "                                                     true_label = test_label_flatten_numerical)\n",
    "            # collect the information\n",
    "            # train\n",
    "            train_precision_epoch_collection.append(precision_array_train) \n",
    "            train_recall_epoch_collection.append(recall_array_train)\n",
    "            train_regret_epoch_collection.append(regret_epoch_train)\n",
    "            # validation\n",
    "            valid_precision_epoch_collection.append(precision_array_valid)\n",
    "            valid_recall_epoch_collection.append(recall_array_valid)\n",
    "            valid_regret_epoch_collection.append(regret_epoch_valid)\n",
    "            # test\n",
    "            test_precision_epoch_collection.append(precision_array_test)\n",
    "            test_recall_epoch_collection.append(recall_array_test)\n",
    "            test_regret_epoch_collection.append(regret_epoch_test)\n",
    "            # print out the information\n",
    "            print('The classification accuracy, train: ', train_data_accuracy, ' validation: ', valid_data_accuracy,' test: ', test_data_accuracy)\n",
    "            print('The train regret of the current epoch is: ', regret_epoch_train)\n",
    "            print('The test regret of the current epoch is: ', regret_epoch_test)\n",
    "            print('Recall Info:')\n",
    "            print('On the training data, the recall of Non-trend:', recall_array_train[0], ' Downtrend:', recall_array_train[1], ' Uptrend:', recall_array_train[2])\n",
    "            print('On the validation data, the recall of Non-trend:', recall_array_valid[0], ' Downtrend:', recall_array_valid[1], ' Uptrend:', recall_array_valid[2])\n",
    "            print('On the testing data, the recall of Non-trend:', recall_array_test[0], ' Downtrend:', recall_array_test[1], ' Uptrend:', recall_array_test[2])\n",
    "            print('Precision Info:')\n",
    "            print('On the training data, the precision of Non-trend:', precision_array_train[0], ' Downtrend:', precision_array_train[1], ' Uptrend:', precision_array_train[2])\n",
    "            print('On the validation data, the precision of Non-trend:', precision_array_valid[0], ' Downtrend:', precision_array_valid[1], ' Uptrend:', precision_array_valid[2])\n",
    "            print('On the testing data, the precision of Non-trend:', precision_array_test[0], ' Downtrend:', precision_array_test[1], ' Uptrend:', precision_array_test[2])\n",
    "            print('Adjusted Precision:')\n",
    "            print('On the training data, Adjusted Downtrend precision: ', precision_downtrend_adjusted_train, 'Adjusted Uptrend precision: ', precision_uptrend_adjuest_train)\n",
    "            print('On the validation data, Adjusted Downtrend precision: ', precision_downtrend_adjusted_valid, 'Adjusted Uptrend precision: ', precision_uptrend_adjuest_valid)\n",
    "            print('On the testing data, Adjusted Downtrend precision: ', precision_downtrend_adjusted_test, 'Adjusted Uptrend precision: ', precision_uptrend_adjuest_test)\n",
    "            print('**************************I\\'m the Divider*****************************')\n",
    "            if(abs((prev_test_loss-currentloss)/currentloss)<=tol):\n",
    "                print('Network stucked to local minimum!')\n",
    "                tf_save_model(sess)\n",
    "                break\n",
    "            # recall of three trends to determine convergence\n",
    "            if (precision_array_valid[0]>=0.50)and(recall_array_valid[1]>=0.15)and(recall_array_valid[2]>=0.15):\n",
    "                save_class_matrix_train = class_matrix_train[:]\n",
    "                save_class_matrix_valid = class_matrix_valid[:]\n",
    "                save_class_matrix_test = class_matrix_test[:]\n",
    "                tf_save_model(sess)\n",
    "                if regret_epoch_train<=0.65:\n",
    "                    print('Network converged!')\n",
    "                    break\n",
    "            # precision of the non-trend as the convergence critaria\n",
    "            downtrend_satis_condition = (abs(precision_array_valid[1]-precision_downtrend_adjusted_valid)/abs(precision_array_valid[1])<1)and(recall_array_valid[1]>=0.10)\n",
    "            uptrend_satis_condition = (abs(precision_array_valid[2]-precision_uptrend_adjuest_valid)/abs(precision_array_valid[2])<1)and(recall_array_valid[2]>=0.10)\n",
    "            if (recall_array_valid[0]>=0.50)and((downtrend_satis_condition)or(uptrend_satis_condition)):\n",
    "                save_class_matrix_train = class_matrix_train[:]\n",
    "                save_class_matrix_valid = class_matrix_valid[:]\n",
    "                save_class_matrix_test = class_matrix_test[:]\n",
    "                tf_save_model(sess)\n",
    "                if regret_epoch_train<=0.65:\n",
    "                    print('Network converged!')\n",
    "                    break\n",
    "            if cEpoch == (nEpochs-1):\n",
    "                print('Network failed to converge!')\n",
    "            prev_test_loss = currentloss\n",
    "    # make the lists as arrays\n",
    "    # train\n",
    "    train_precision_epoch_array = np.array(train_precision_epoch_collection)\n",
    "    train_recall_epoch_array = np.array(train_recall_epoch_collection)\n",
    "    train_regret_epoch_array = np.array(train_regret_epoch_collection)\n",
    "    # put them into one list to save \n",
    "    train_epoch_wise_info = [train_precision_epoch_array, train_recall_epoch_array, train_regret_epoch_array, save_class_matrix_train]\n",
    "    # validation\n",
    "    valid_precision_epoch_array = np.array(valid_precision_epoch_collection)\n",
    "    valid_recall_epoch_array = np.array(valid_recall_epoch_collection)\n",
    "    valid_regret_epoch_array = np.array(valid_regret_epoch_collection)\n",
    "    # put them into one list to save \n",
    "    valid_epoch_wise_info = [valid_precision_epoch_array, valid_recall_epoch_array, valid_regret_epoch_array, save_class_matrix_valid]\n",
    "    # test\n",
    "    test_precision_epoch_array = np.array(test_precision_epoch_collection)\n",
    "    test_recall_epoch_array = np.array(test_recall_epoch_collection)\n",
    "    test_regret_epoch_array = np.array(test_regret_epoch_collection)\n",
    "    # put them into one list to save\n",
    "    test_epoch_wise_info = [test_precision_epoch_array, test_recall_epoch_array, test_regret_epoch_array, save_class_matrix_test]\n",
    "    \n",
    "    return train_epoch_wise_info, valid_epoch_wise_info, test_epoch_wise_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1 th out of  100 Epochs in total has finished and loss in this epoch is 9193247.464660645\n",
      "The classification loss: training:  786.09906  validation:  2028.539  test:  3974.642\n",
      "The regression loss: training:  18410.271  validation:  46490.402  test:  73561.7\n",
      "The fluctuation loss: training:  39593.746  validation:  81086.055  test:  136162.69\n",
      "The classification accuracy, train:  0.41157484  validation:  0.48193267  test:  0.4305913\n",
      "The train regret of the current epoch is:  0.6779481616907371\n",
      "The test regret of the current epoch is:  0.664653760673763\n",
      "Recall Info:\n",
      "On the training data, the recall of Non-trend: 0.49013956310679613  Downtrend: 0.09129332206255283  Uptrend: 0.4022654309770428\n",
      "On the validation data, the recall of Non-trend: 0.7176627022235355  Downtrend: 0.28227647988758126  Uptrend: 0.0029349720260478766\n",
      "On the testing data, the recall of Non-trend: 0.6708844580777096  Downtrend: 0.33284226033884035  Uptrend: 0.0015681116900197278\n",
      "Precision Info:\n",
      "On the training data, the precision of Non-trend: 0.5786071166347307  Downtrend: 0.1331815594763802  Uptrend: 0.2770893311636834\n",
      "On the validation data, the precision of Non-trend: 0.6253620856131317  Downtrend: 0.12873507970840342  Uptrend: 0.27350427350427353\n",
      "On the testing data, the precision of Non-trend: 0.5727973111591261  Downtrend: 0.14079679501446696  Uptrend: 0.28703703703703703\n",
      "Adjusted Precision:\n",
      "On the training data, Adjusted Downtrend precision:  -0.1769114020110036 Adjusted Uptrend precision:  0.13291192055743314\n",
      "On the validation data, Adjusted Downtrend precision:  -0.13594488504365937 Adjusted Uptrend precision:  0.16239316239316243\n",
      "On the testing data, Adjusted Downtrend precision:  -0.14778544402403737 Adjusted Uptrend precision:  0.09259259259259259\n",
      "**************************I'm the Divider*****************************\n",
      "The 2 th out of  100 Epochs in total has finished and loss in this epoch is 232427.33157348633\n",
      "The classification loss: training:  254.53593  validation:  587.3704  test:  1161.0897\n",
      "The regression loss: training:  9235.559  validation:  28359.24  test:  34670.4\n",
      "The fluctuation loss: training:  25090.613  validation:  52722.734  test:  70834.06\n",
      "The classification accuracy, train:  0.395453  validation:  0.26693383  test:  0.25709146\n",
      "The train regret of the current epoch is:  0.7009018781108671\n",
      "The test regret of the current epoch is:  0.9449204585331618\n",
      "Recall Info:\n",
      "On the training data, the recall of Non-trend: 0.4668992718446602  Downtrend: 0.12419533129592301  Uptrend: 0.3820643285480647\n",
      "On the validation data, the recall of Non-trend: 0.21670237127871758  Downtrend: 0.5461092569822589  Uptrend: 0.2526827478675594\n",
      "On the testing data, the recall of Non-trend: 0.19013292433537832  Downtrend: 0.5870777649163422  Uptrend: 0.2307147554251606\n",
      "Precision Info:\n",
      "On the training data, the precision of Non-trend: 0.5748706550365155  Downtrend: 0.11756740120645082  Uptrend: 0.2832794395041768\n",
      "On the validation data, the precision of Non-trend: 0.644867003737085  Downtrend: 0.13339912468892132  Uptrend: 0.24454109710633765\n",
      "On the testing data, the precision of Non-trend: 0.582961047104005  Downtrend: 0.14054312777106007  Uptrend: 0.28618937064692224\n",
      "Adjusted Precision:\n",
      "On the training data, Adjusted Downtrend precision:  -0.19715622307029423 Adjusted Uptrend precision:  0.14858977813706997\n",
      "On the validation data, Adjusted Downtrend precision:  -0.12915129151291513 Adjusted Uptrend precision:  0.12187111663412036\n",
      "On the testing data, Adjusted Downtrend precision:  -0.15303808948004835 Adjusted Uptrend precision:  0.15084394804542886\n",
      "**************************I'm the Divider*****************************\n",
      "The 3 th out of  100 Epochs in total has finished and loss in this epoch is 89493.91067504883\n",
      "The classification loss: training:  148.21304  validation:  347.82056  test:  674.1547\n",
      "The regression loss: training:  7016.7935  validation:  10652.253  test:  23237.402\n",
      "The fluctuation loss: training:  12672.451  validation:  31246.951  test:  41930.504\n",
      "The classification accuracy, train:  0.45702738  validation:  0.5035722  test:  0.4784624\n",
      "The train regret of the current epoch is:  0.6110027380004723\n",
      "The test regret of the current epoch is:  0.5728009123874137\n",
      "Recall Info:\n",
      "On the training data, the recall of Non-trend: 0.6589805825242718  Downtrend: 0.144027570063073  Uptrend: 0.20125386153007452\n",
      "On the validation data, the recall of Non-trend: 0.7077639063307971  Downtrend: 0.10837871069734761  Uptrend: 0.20315509492800146\n",
      "On the testing data, the recall of Non-trend: 0.7270705521472393  Downtrend: 0.1085972850678733  Uptrend: 0.1613637513278365\n",
      "Precision Info:\n",
      "On the training data, the precision of Non-trend: 0.5780746812870945  Downtrend: 0.13430754305117631  Uptrend: 0.2930023369637109\n",
      "On the validation data, the precision of Non-trend: 0.6216987865810135  Downtrend: 0.13283100107642626  Uptrend: 0.2700231622577106\n",
      "On the testing data, the precision of Non-trend: 0.568348486362274  Downtrend: 0.14345287739783152  Uptrend: 0.2860216981977943\n",
      "Adjusted Precision:\n",
      "On the training data, Adjusted Downtrend precision:  -0.1605626970652438 Adjusted Uptrend precision:  0.16213236915207901\n",
      "On the validation data, Adjusted Downtrend precision:  -0.1222820236813778 Adjusted Uptrend precision:  0.15799097891015484\n",
      "On the testing data, Adjusted Downtrend precision:  -0.13094245204336946 Adjusted Uptrend precision:  0.15385994799605487\n",
      "**************************I'm the Divider*****************************\n",
      "Network converged!\n"
     ]
    }
   ],
   "source": [
    "train_epoch_wise_info, valid_epoch_wise_info, test_epoch_wise_info = train_recurrent_neural_network(x_time_series, x_SP_data, x_time_series_prev, x_SP_data_prev, x_current_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = '../results_time_split/'\n",
    "# make the path if it does not exist yet\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "# save training\n",
    "np.save(save_path+'train_precision_epoch_array.npy',train_epoch_wise_info[0])\n",
    "np.save(save_path+'train_recall_epoch_array.npy',train_epoch_wise_info[1]) \n",
    "np.save(save_path+'train_regret_epoch_array.npy',train_epoch_wise_info[2]) \n",
    "np.save(save_path+'save_class_matrix_train.npy',train_epoch_wise_info[3]) \n",
    "# save validation\n",
    "np.save(save_path+'valid_precision_epoch_array.npy',valid_epoch_wise_info[0])\n",
    "np.save(save_path+'valid_recall_epoch_array.npy',valid_epoch_wise_info[1]) \n",
    "np.save(save_path+'valid_regret_epoch_array.npy',valid_epoch_wise_info[2]) \n",
    "np.save(save_path+'save_class_matrix_valid.npy',valid_epoch_wise_info[3]) \n",
    "# save the testing\n",
    "np.save(save_path+'test_precision_epoch_array.npy',test_epoch_wise_info[0])\n",
    "np.save(save_path+'test_recall_epoch_array.npy',test_epoch_wise_info[1]) \n",
    "np.save(save_path+'test_regret_epoch_array.npy',test_epoch_wise_info[2]) \n",
    "np.save(save_path+'save_class_matrix_test.npy',test_epoch_wise_info[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}