{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from Metric_Computation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify the path to read\n",
    "data_load_path = '../data/intermediate/'\n",
    "# read the data\n",
    "data_input_X = np.load(data_load_path+'data_X.npy')\n",
    "data_input_X_prev = np.load(data_load_path+'data_X_prev.npy')\n",
    "SP500_input_X = np.load(data_load_path+'SP500.npy')\n",
    "SP500_input_X_prev = np.load(data_load_path+'SP500_prev.npy')\n",
    "value_target = np.load(data_load_path+'target_value.npy')\n",
    "value_target_prev = np.load(data_load_path+'target_value_prev.npy')\n",
    "gradient_target = np.load(data_load_path+'target_gradient.npy')\n",
    "gradient_target_prev = np.load(data_load_path+'target_gradient_prev.npy')\n",
    "trend_target = np.load(data_load_path+'price_trend_flag.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop some really weird stock gradient entries\n",
    "# valid_stock_index = np.where((np.absolute(gradient_target)<=7.0)&(np.absolute(gradient_target_prev)<=7.0))[0]\n",
    "# data_input_X = data_input_X[valid_stock_index,:,:]\n",
    "# data_input_X_prev = data_input_X_prev[valid_stock_index,:,:]\n",
    "# SP500_input_X = SP500_input_X[valid_stock_index,:]\n",
    "# SP500_input_X_prev = SP500_input_X_prev[valid_stock_index,:]\n",
    "# value_target = value_target[valid_stock_index,:]\n",
    "# value_target_prev = value_target_prev[valid_stock_index,:]\n",
    "# gradient_target = gradient_target[valid_stock_index,:]\n",
    "# gradient_target_prev = gradient_target_prev[valid_stock_index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop instance that contains inf values\n",
    "valid_series_index = np.where((np.sum(np.sum(data_input_X,axis=-1),axis=-1)!=np.inf)&((np.sum(np.sum(data_input_X_prev,axis=-1),axis=-1)!=np.inf)))[0]\n",
    "data_input_X = data_input_X[valid_series_index,:,:]\n",
    "data_input_X_prev = data_input_X_prev[valid_series_index,:,:]\n",
    "SP500_input_X = SP500_input_X[valid_series_index,:]\n",
    "SP500_input_X_prev = SP500_input_X_prev[valid_series_index,:]\n",
    "value_target = value_target[valid_series_index,:]\n",
    "value_target_prev = value_target_prev[valid_series_index,:]\n",
    "gradient_target = gradient_target[valid_series_index,:]\n",
    "gradient_target_prev = gradient_target_prev[valid_series_index,:]\n",
    "trend_target = trend_target[valid_series_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data\n",
    "ind_permutation = np.random.permutation(data_input_X.shape[0])\n",
    "# training and testing index\n",
    "training_ind = ind_permutation[:ind_permutation.shape[0]*3//4]\n",
    "testing_ind = ind_permutation[ind_permutation.shape[0]*3//4:]\n",
    "# indexing the data\n",
    "# time series data\n",
    "data_train_X = data_input_X[training_ind,:,:]\n",
    "data_train_X_prev = data_input_X_prev[training_ind,:,:]\n",
    "data_test_X = data_input_X[testing_ind,:,:]\n",
    "data_test_X_prev = data_input_X_prev[testing_ind,:,:]\n",
    "# SP500 info\n",
    "SP500_train_X = np.reshape(SP500_input_X[training_ind],[-1,1])\n",
    "SP500_train_X_prev = np.reshape(SP500_input_X_prev[training_ind],[-1,1])\n",
    "SP500_test_X = np.reshape(SP500_input_X[testing_ind],[-1,1])\n",
    "SP500_test_X_prev = np.reshape(SP500_input_X_prev[testing_ind],[-1,1])\n",
    "# value prediction (for reference usage only)\n",
    "value_target_train = np.reshape(value_target[training_ind],[-1,1])\n",
    "value_target_train_prev = np.reshape(value_target_prev[training_ind],[-1,1])\n",
    "value_target_test = np.reshape(value_target[testing_ind],[-1,1])\n",
    "value_target_test_prev = np.reshape(value_target_prev[testing_ind],[-1,1])\n",
    "# gradient prediction (the real 'labels' for training)\n",
    "gradient_target_train = np.reshape(gradient_target[training_ind],[-1,1])\n",
    "gradient_target_train_prev = np.reshape(gradient_target_prev[training_ind],[-1,1])\n",
    "gradient_target_test = np.reshape(gradient_target[testing_ind],[-1,1])\n",
    "gradient_target_test_prev = np.reshape(gradient_target_prev[testing_ind],[-1,1])\n",
    "# train and test trend info\n",
    "trend_target_train = np.reshape(trend_target[training_ind],[-1,1])\n",
    "trend_target_test = np.reshape(trend_target[testing_ind],[-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_and_future_price_train = np.concatenate([value_target_train_prev, value_target_train],axis=1)\n",
    "current_and_future_price_test = np.concatenate([value_target_test_prev, value_target_test],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preceed the trend(class) target to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding_label(label_input, num_class=None):\n",
    "    '''\n",
    "    :param label_input: The sparse form of input label (2,0,1,3,0,1,2etc.)\n",
    "    :param num_class: The number of classes, if keep None, then automatically infer from the given label input\n",
    "    '''\n",
    "    # retrieve the number of input data\n",
    "    nData = label_input.shape[0]\n",
    "    # reshape the data\n",
    "    label_input_flat = np.reshape(label_input, [-1])\n",
    "    if (label_input_flat.shape[0]!=nData):            # which means the input label is not 'mathematically 1-d'\n",
    "        raise ValueError('The input label must be 1-d mathematically')\n",
    "    # infer the number of class if input is None\n",
    "    if num_class is None:\n",
    "        num_class = (int)(np.amax(label_input)+1)\n",
    "    # create the return encoded matrx\n",
    "    one_hot_label_mat = np.zeros([nData, num_class])\n",
    "    # get a row index to assist the batch-assigning\n",
    "    row_ind_batch = np.arange(nData)\n",
    "    # assign '1's to the corresponding positions\n",
    "    one_hot_label_mat[row_ind_batch, label_input_flat.astype('int')] = 1\n",
    "    \n",
    "    return one_hot_label_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_target_train_input = one_hot_encoding_label(trend_target_train)\n",
    "trend_target_test_input = one_hot_encoding_label(trend_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# confusion metrics\n",
    "def classification_info_computation(pred_label, true_label, num_class):\n",
    "    '''\n",
    "    :param pred_label: the sparse (not one-hot) prediction of labels\n",
    "    :param true_label: the sparse (not one-hot) ground-truth of labels\n",
    "    :param num_class: number of classes\n",
    "    '''\n",
    "    # flatten the two label arrays if they are not already so\n",
    "    pred_label = np.reshape(pred_label,[-1])\n",
    "    true_label = np.reshape(true_label,[-1])\n",
    "    # initialize the confusion maxtrix array\n",
    "    class_matrix = np.zeros([num_class, num_class])    # each row is the true labels\n",
    "    # initialize the precision and recall arrays\n",
    "    precision_array = np.zeros([num_class])\n",
    "    recall_array = np.zeros([num_class])\n",
    "    # fill the confusion-prediction matrix\n",
    "    for cClass_True in range(num_class):\n",
    "        # retrieve the current \n",
    "        current_cClass_ind = np.where(true_label==cClass_True)[0]\n",
    "        # retrueve the corresponding predictions\n",
    "        current_cClass_pred = pred_label[current_cClass_ind]\n",
    "        # fill the evaluation matrx\n",
    "        for cClass_Pred in range(num_class):\n",
    "            cClass_pred_num = np.where(current_cClass_pred==cClass_Pred)[0].shape[0]\n",
    "            class_matrix[cClass_True, cClass_Pred] = cClass_pred_num\n",
    "    # fill the precision and recall arrays\n",
    "    for cClass_True in range(num_class):\n",
    "        precision_array[cClass_True] = class_matrix[cClass_True,cClass_True]/np.sum(class_matrix[:,cClass_True])\n",
    "        recall_array[cClass_True] = class_matrix[cClass_True,cClass_True]/np.sum(class_matrix[cClass_True,:])\n",
    "        \n",
    "    return class_matrix, precision_array, recall_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'regret' function\n",
    "def invest_regret_comput(pred_label, true_label):\n",
    "    '''\n",
    "    The function to return the 'regret' defined by the real investment scenarios\n",
    "    :param pred_label: the sparse (not one-hot) prediction of labels\n",
    "    :param true_label: the sparse (not one-hot) ground-truth of labels\n",
    "    With the meaning 2=uptrend 1=downtrend 0=non-trend\n",
    "    Strategy: \n",
    "        predict 0: don't buy or sell\n",
    "        predict 1: sell\n",
    "        predict 2: buy\n",
    "    ******************** Truth Table *********************\n",
    "    | True Label | Predicted Label | Regret |\n",
    "    |      0     |        0        |   0    |\n",
    "    |      0     |        1        |   1    |\n",
    "    |      0     |        2        |   1    |\n",
    "    |      1     |        0        |   1    |\n",
    "    |      1     |        1        |   0    |\n",
    "    |      1     |        2        |   2    |\n",
    "    |      2     |        0        |   1    |\n",
    "    |      2     |        1        |   2    |\n",
    "    |      2     |        2        |   0    |\n",
    "    '''\n",
    "    # flatten the two label arrays if they are not already so\n",
    "    pred_label = np.reshape(pred_label,[-1])\n",
    "    true_label = np.reshape(true_label,[-1])\n",
    "    # check if the two arrays are of the same legth\n",
    "    if pred_label.shape[0]!=true_label.shape[0]:\n",
    "        raise ValueError('The predicted and the true labels must be in the same length!')\n",
    "    # placeholder of regret array\n",
    "    regret_array = np.zeros([pred_label.shape[0]])\n",
    "    # check the conditions for regret '1'\n",
    "    one_regret_ind = np.where(((true_label==0)&(pred_label==2))|((true_label==0)&(pred_label==1))|((true_label==2)&(pred_label==0))|((true_label==1)&(pred_label==0)))[0]\n",
    "    # check the conditions for regret '2'\n",
    "    two_regret_ind = np.where(((true_label==1)&(pred_label==2))|((true_label==2)&(pred_label==1)))[0]\n",
    "    # assign regret values to the entries\n",
    "    regret_array[one_regret_ind] = 1.0\n",
    "    regret_array[two_regret_ind] = 2.0\n",
    "    # compute the overall regret\n",
    "    overall_regret = np.mean(regret_array)\n",
    "    \n",
    "    return overall_regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters of the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rnn cell sequence length and dimensions\n",
    "nChunck = data_input_X.shape[1]\n",
    "nDim = data_input_X.shape[2]\n",
    "# output dimension\n",
    "nClass = 3\n",
    "# number of nested RNN stack\n",
    "nStack_rnn = 5\n",
    "# rnn dimension\n",
    "rnn_size = 64\n",
    "# training hyper-parameter\n",
    "nEpochs = 100\n",
    "batch_size = 256\n",
    "pred_loss_coeff = 0.5\n",
    "l2_FC_pen_coeff = 1e-3\n",
    "fluc_pen_coeff = 1e-3\n",
    "learning_rate = 1e-4\n",
    "tol = 1e-3\n",
    "focal_gamma = 3.0     # parameter for the focal loss\n",
    "trend_weight_alpha = 2.0  # parameter to encourage output 1/2\n",
    "# data info for feeding them\n",
    "nData_train = data_train_X.shape[0]\n",
    "nData_test = data_test_X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A 'Erease Graph' here to enable us to re-run things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Model_path = '../model/trend_bidirec_RNN/'\n",
    "#Model saving function\n",
    "def tf_save_model(session):\n",
    "    if not os.path.exists(Model_path):\n",
    "        os.makedirs(Model_path)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(session, Model_path+'Bidirec_RNN.checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the RNN graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the x input as float and reshape input\n",
    "x_time_series = tf.placeholder(\"float\",[None,nChunck,nDim])       # Batch_size * n_chunk * chunk_size\n",
    "x_time_series_prev = tf.placeholder(\"float\",[None,nChunck,nDim])  # Batch_size * n_chunk * chunk_size\n",
    "x_SP_data = tf.placeholder(\"float\",[None,1])                      # Batch_size * 1\n",
    "x_SP_data_prev = tf.placeholder(\"float\",[None,1])                 # Batch_size * 1\n",
    "x_current_price = tf.placeholder(\"float\",[None,2])                # Batch_size * 2\n",
    "y = tf.placeholder(\"float\",[None,1])                              # Batch_size * 1\n",
    "y_prev = tf.placeholder(\"float\",[None,1])                         # Batch_size * 1\n",
    "y_trend = tf.placeholder(\"float\",[None,nClass])                   # Batch_size * n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def swish_activation(data_input):\n",
    "    '''\n",
    "    Swish activation, by Ramachandran et al., Google Brain, 2017\n",
    "    The activation is found by a RNN-based combinational search, and it has been proved consistently outperforming\n",
    "        RELU on networks like NASNet-A and Inception-ResNet-v2\n",
    "    '''\n",
    "    # define the beta variable as trainable\n",
    "    para_beta = tf.Variable(tf.random_uniform([1]))\n",
    "    swish_output = data_input*tf.sigmoid(para_beta*data_input)\n",
    "    \n",
    "    return swish_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Recurrent_neural_network(x_time_series, x_SP_data, x_time_series_prev, x_SP_data_prev, x_current_price):\n",
    "    # process the data concatenating the current and previous data\n",
    "    data_processed = tf.concat([x_time_series,x_time_series_prev],axis=0)\n",
    "    # split the data into (nChunck * [batch_size, nDim])       \n",
    "    data_processed = tf.split(data_processed, num_or_size_splits=nChunck, axis=1)  # (nChunck * [batch_size, nDim])\n",
    "    data_processed = [tf.squeeze(this_time_data, axis=1) for this_time_data in data_processed]\n",
    "    # define individual GRU cells\n",
    "    # forward\n",
    "    stack_GRU_cell_forward = []\n",
    "    for _ in range(nStack_rnn):\n",
    "        current_rnn_cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.GRUCell(rnn_size),\n",
    "                                                         input_keep_prob=0.7,\n",
    "                                                         variational_recurrent=False)# ,\n",
    "#                                                          input_size=nDim,\n",
    "#                                                          dtype=tf.float32)\n",
    "        stack_GRU_cell_forward.append(current_rnn_cell)\n",
    "    heiarchy_RNN_forward = tf.contrib.rnn.MultiRNNCell(stack_GRU_cell_forward)\n",
    "    # backward\n",
    "    stack_GRU_cell_backward = []\n",
    "    for _ in range(nStack_rnn):\n",
    "        current_rnn_cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.GRUCell(rnn_size),\n",
    "                                                         input_keep_prob=0.7,\n",
    "                                                         variational_recurrent=False)#,\n",
    "#                                                          input_size=nDim,\n",
    "#                                                          dtype=tf.float32)\n",
    "        stack_GRU_cell_backward.append(current_rnn_cell)\n",
    "    heiarchy_RNN_backward = tf.contrib.rnn.MultiRNNCell(stack_GRU_cell_backward)\n",
    "    # output of the forward-backword RNN\n",
    "    outputs_all, output_for_final, output_back_final = tf.contrib.rnn.static_bidirectional_rnn(cell_fw=heiarchy_RNN_forward,\n",
    "                                                                                               cell_bw=heiarchy_RNN_backward,\n",
    "                                                                                               inputs=data_processed,\n",
    "                                                                                               dtype=tf.float32)\n",
    "    # Post-RNN process\n",
    "    # concatenate the current and prev S&P-500\n",
    "    x_SP_data_processed = tf.concat([x_SP_data,x_SP_data_prev],axis=0)\n",
    "    # [nBatch x 2*rnn_stack*rnn_size+1]\n",
    "    # print(outputs_all[-1].get_shape().as_list())\n",
    "    out_status_processed = tf.concat([outputs_all[-1], x_SP_data_processed],axis=1)   # [batch_size x (2*rnn_size+1)]\n",
    "    with tf.name_scope(\"FC_layers_rate\"):\n",
    "        weight_FC_rate = {'weight_hidden': tf.Variable(tf.random_normal([2*rnn_size+1, 64])),\n",
    "                     'weight_out': tf.Variable(tf.random_normal([64, 1]))}\n",
    "    with tf.name_scope(\"FC_layers_scale\"):\n",
    "        weight_FC_scale = {'weight_hidden': tf.Variable(tf.random_normal([2*rnn_size+1, 64])),\n",
    "                     'weight_out': tf.Variable(tf.random_normal([64, 1]))}\n",
    "    with tf.name_scope(\"FC_layers_class\"):\n",
    "        weight_FC_class = {'weight_hidden_1': tf.Variable(tf.random_normal([2*rnn_size+4, 128])),\n",
    "                           'weight_hidden_2': tf.Variable(tf.random_normal([128, 64])),\n",
    "                           'weight_out': tf.Variable(tf.random_normal([64, nClass]))}\n",
    "    # ***************rate prediction**************\n",
    "    # fully-connected hidden\n",
    "    process_output_rate = tf.matmul(out_status_processed, weight_FC_rate[\"weight_hidden\"])    # [batch_size x 64]\n",
    "    # non-linear activation \n",
    "    process_output_rate = swish_activation(process_output_rate)\n",
    "    process_output_rate = tf.nn.dropout(process_output_rate, keep_prob = 0.7)\n",
    "    #         process_output = tf.nn.tanh(process_output)\n",
    "    # process to the final output\n",
    "    final_output_rate = tf.nn.sigmoid(tf.matmul(process_output_rate, weight_FC_rate[\"weight_out\"]))  # [batch_size x 1]\n",
    "    # ***************scale prediction**************\n",
    "    process_output_scale = tf.matmul(out_status_processed, weight_FC_scale[\"weight_hidden\"])\n",
    "    # non-linear activation \n",
    "    process_output_scale = swish_activation(process_output_scale)\n",
    "    process_output_scale = tf.nn.dropout(process_output_scale, keep_prob = 0.7)\n",
    "    # out\n",
    "    # pass to RELU to limit the learning inside the regime of scaling\n",
    "    final_output_scale = tf.nn.relu(tf.matmul(process_output_scale, weight_FC_scale[\"weight_out\"]))\n",
    "    # combine them to get the final output\n",
    "    final_output = tf.multiply(final_output_rate, final_output_scale)\n",
    "    # split the results to get the prediction for current and previous data\n",
    "    current_prediction, prev_prediction = tf.split(final_output,num_or_size_splits=2,axis=0)\n",
    "    # ***************class prediction****************\n",
    "    out_status_processed_class = tf.concat([tf.split(out_status_processed,num_or_size_splits=2,axis=0)[0],  # only pick the 'current'\n",
    "                                            x_current_price,\n",
    "                                            current_prediction],axis=1)\n",
    "    # layer 1\n",
    "    process_output_class = tf.matmul(out_status_processed_class, weight_FC_class[\"weight_hidden_1\"])\n",
    "    process_output_class = swish_activation(process_output_class)\n",
    "    # layer 2\n",
    "    process_output_class = tf.matmul(process_output_class, weight_FC_class[\"weight_hidden_2\"])\n",
    "    process_output_class = swish_activation(process_output_class)\n",
    "    # layer 3 (out-layer)\n",
    "    final_output_class = tf.matmul(process_output_class, weight_FC_class[\"weight_out\"])            \n",
    "\n",
    "    return current_prediction, prev_prediction, final_output_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def focal_classification_loss(logits, labels, trend_weight):\n",
    "    '''\n",
    "    will be resemble the cross-entropy calssification loss, but with the focal loss-decrease term for well-classes \n",
    "    examples to tackle the problem of imbalance data.\n",
    "    :param logits: The [nData * nClass] un-normalised prediction\n",
    "    :param labels: The [nData * nClass] one-hot true labels\n",
    "    :param trend_weight: To encourage the network to predict up/down trends. Should be specify\n",
    "    :return: the loss of [nData] shape\n",
    "    ''' \n",
    "    # get the pobabilistic sotmax output\n",
    "    soft_max_prediction = tf.nn.softmax(logits, axis=1)\n",
    "    # process to log(soft_max) output, use this integrated function to faciliate numerical stability\n",
    "    log_soft_max_prediction = tf.nn.log_softmax(logits, axis=1)\n",
    "    # compute focal loss\n",
    "    class_weight = tf.constant([1.0, trend_weight, trend_weight])\n",
    "    # -sum(y*(1-p)^(gamma)*log(p))  \n",
    "    focal_loss = -tf.reduce_sum(tf.multiply(tf.multiply(labels,tf.multiply(tf.pow(1-soft_max_prediction,focal_gamma),\n",
    "                                                                           log_soft_max_prediction)),class_weight),\n",
    "                                axis=1)\n",
    "    \n",
    "    return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_recurrent_neural_network(x_time_series, x_SP_data, x_time_series_prev, x_SP_data_prev, x_current_price):\n",
    "    # get the prediction\n",
    "    grad_pred, grad_pred_prev, class_pred = Recurrent_neural_network(x_time_series, x_SP_data, x_time_series_prev, x_SP_data_prev, x_current_price)\n",
    "    # mean square loss\n",
    "    loss_prediction = tf.losses.mean_squared_error(predictions=grad_pred, labels=y)\n",
    "    # compute the loss of fluctuation\n",
    "    # use this scheme to back-prop gradient to both sides\n",
    "    loss_fluctuation = (tf.losses.mean_squared_error(predictions=grad_pred, labels=grad_pred_prev) + \n",
    "                        tf.losses.mean_squared_error(predictions=grad_pred_prev, labels=grad_pred))/2\n",
    "    # mis-classification loss\n",
    "    # loss_classification = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=class_pred, labels=y_trend))\n",
    "    # focal loss\n",
    "    loss_classification = tf.reduce_mean(focal_classification_loss(logits=class_pred, labels=y_trend, trend_weight=trend_weight_alpha))\n",
    "    # l_2 loss for FC layers\n",
    "    reg_vars = (tf.trainable_variables(scope='FC_layers_rate') \n",
    "                + tf.trainable_variables(scope='FC_layers_scale') \n",
    "                + tf.trainable_variables(scope='FC_layers_class'))\n",
    "    loss_l2_FC = tf.add_n([tf.nn.l2_loss(var) for var in reg_vars])\n",
    "    # overall loss\n",
    "    loss = loss_classification + pred_loss_coeff*loss_prediction + l2_FC_pen_coeff*loss_l2_FC + fluc_pen_coeff*loss_fluctuation\n",
    "    # defineing optimiser\n",
    "    optimiser_org = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    # apply gradient clipping to avoid gradient exploding\n",
    "    gradients_nn = optimiser_org.compute_gradients(loss)\n",
    "    clipped_grad = [(tf.clip_by_value(grad, -10., 10.), var) for grad, var in gradients_nn]\n",
    "    optimiser = optimiser_org.apply_gradients(clipped_grad)\n",
    "    prev_test_loss = 0\n",
    "    # evaluation tensor\n",
    "    label_flatten_tensor = tf.argmax(y_trend, 1)\n",
    "    logit_flatten_tensor = tf.argmax(class_pred,1)\n",
    "    acc_tensor = tf.reduce_mean(tf.cast(tf.equal(label_flatten_tensor, logit_flatten_tensor),tf.float32))\n",
    "    # the training dictionary\n",
    "    train_dict = {x_time_series: data_train_X,\n",
    "                  x_time_series_prev: data_train_X_prev,\n",
    "                  x_SP_data: SP500_train_X,\n",
    "                  x_SP_data_prev: SP500_train_X_prev,\n",
    "                  y: gradient_target_train,\n",
    "                  y_prev: gradient_target_train_prev, \n",
    "                  y_trend: trend_target_train_input,\n",
    "                  x_current_price: current_and_future_price_train}\n",
    "    # the test dictionary\n",
    "    test_dict = {x_time_series: data_test_X,\n",
    "                 x_time_series_prev: data_test_X_prev,\n",
    "                 x_SP_data: SP500_test_X,\n",
    "                 x_SP_data_prev: SP500_test_X_prev,\n",
    "                 y: gradient_target_test,\n",
    "                 y_prev: gradient_target_test_prev, \n",
    "                 y_trend: trend_target_test_input, \n",
    "                 x_current_price: current_and_future_price_test}\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # define the lists to collect the infomation while training\n",
    "        # train\n",
    "        train_precision_epoch_collection = []\n",
    "        train_recall_epoch_collection = []\n",
    "        train_regret_epoch_collection = []\n",
    "        # test\n",
    "        test_precision_epoch_collection = []\n",
    "        test_recall_epoch_collection = []\n",
    "        test_regret_epoch_collection = []\n",
    "        for cEpoch in range(nEpochs):\n",
    "            current_Epoch_Loss = 0\n",
    "            random_index = np.random.choice(nData_train, size=nData_train)    # Use this to avoid overflow for the derivative index\n",
    "            for i in range(nData_train//batch_size):\n",
    "                this_index = random_index[i*batch_size:(i+1)*batch_size]\n",
    "                current_x_time_series = data_train_X[this_index]\n",
    "                current_x_time_series_prev = data_train_X_prev[this_index]\n",
    "                current_x_SP_500 = SP500_train_X[this_index]\n",
    "                current_x_SP_500_prev = SP500_train_X_prev[this_index]\n",
    "                current_x_price_info = current_and_future_price_train[this_index]\n",
    "                current_y_gradient = gradient_target_train[this_index]\n",
    "                current_y_gradient_prev = gradient_target_train_prev[this_index]\n",
    "                current_y_trend = trend_target_train_input[this_index]\n",
    "                #Get how many batches we need for each epoch\n",
    "                this_feed_dict = {x_time_series: current_x_time_series,\n",
    "                                  x_time_series_prev: current_x_time_series_prev,\n",
    "                                  x_SP_data: current_x_SP_500, \n",
    "                                  x_SP_data_prev: current_x_SP_500_prev,\n",
    "                                  y: current_y_gradient, \n",
    "                                  y_prev: current_y_gradient_prev,\n",
    "                                  y_trend: current_y_trend, \n",
    "                                  x_current_price: current_x_price_info}\n",
    "                _, currentloss = sess.run([optimiser,loss], feed_dict = this_feed_dict)\n",
    "#                 print(currentloss)\n",
    "#                 print(np.amax(current_x_time_series))\n",
    "#                 print(np.amin(current_x_time_series))\n",
    "                current_Epoch_Loss += currentloss\n",
    "                # break\n",
    "            print('The',cEpoch+1,'th out of ',nEpochs,'Epochs in total has finished and loss in this epoch is',current_Epoch_Loss)\n",
    "            # Check the l2 loss for training and testing data\n",
    "            train_data_loss = loss_prediction.eval(feed_dict=train_dict)\n",
    "            test_data_loss = loss_prediction.eval(feed_dict=test_dict)\n",
    "            # Also check the current classification accuracy\n",
    "            train_data_accuracy = acc_tensor.eval(feed_dict=train_dict)\n",
    "            test_data_accuracy = acc_tensor.eval(feed_dict=test_dict)\n",
    "            # evluation and accuracy\n",
    "            # flattened predictions\n",
    "            train_logit_flatten_numerical = logit_flatten_tensor.eval(train_dict)\n",
    "            train_label_flatten_numerical = label_flatten_tensor.eval(train_dict)\n",
    "            test_logit_flatten_numerical = logit_flatten_tensor.eval(test_dict)\n",
    "            test_label_flatten_numerical = label_flatten_tensor.eval(test_dict)\n",
    "            # train\n",
    "            class_matrix_train, precision_array_train, recall_array_train = classification_info_computation(pred_label = train_logit_flatten_numerical, \n",
    "                                                                                                            true_label = train_label_flatten_numerical, \n",
    "                                                                                                            num_class=3)\n",
    "            down_mis_up_rate_train = class_matrix_train[2,1]/np.sum(class_matrix_train[:,1])\n",
    "            up_mis_down_rate_train = class_matrix_train[1,2]/np.sum(class_matrix_train[:,2])\n",
    "            precision_downtrend_adjusted_train = precision_array_train[1] - down_mis_up_rate_train\n",
    "            precision_uptrend_adjuest_train = precision_array_train[2] - up_mis_down_rate_train\n",
    "            # test\n",
    "            class_matrix_test, precision_array_test, recall_array_test = classification_info_computation(pred_label = test_logit_flatten_numerical, \n",
    "                                                                                                         true_label = test_label_flatten_numerical, \n",
    "                                                                                                         num_class=3)\n",
    "            down_mis_up_rate_test = class_matrix_test[2,1]/np.sum(class_matrix_test[:,1])\n",
    "            up_mis_down_rate_test = class_matrix_test[1,2]/np.sum(class_matrix_test[:,2])\n",
    "            precision_downtrend_adjusted_test = precision_array_test[1] - down_mis_up_rate_test\n",
    "            precision_uptrend_adjuest_test = precision_array_test[2] - up_mis_down_rate_test\n",
    "            # training and testing regret\n",
    "            regret_epoch_train = invest_regret_comput(pred_label = train_logit_flatten_numerical,\n",
    "                                                      true_label = train_label_flatten_numerical)\n",
    "            regret_epoch_test = invest_regret_comput(pred_label = test_logit_flatten_numerical,\n",
    "                                                     true_label = test_label_flatten_numerical)\n",
    "            # collect the information\n",
    "            train_precision_epoch_collection.append(precision_array_train) \n",
    "            train_recall_epoch_collection.append(recall_array_train)\n",
    "            train_regret_epoch_collection.append(regret_epoch_train)\n",
    "            # test\n",
    "            test_precision_epoch_collection.append(precision_array_test)\n",
    "            test_recall_epoch_collection.append(recall_array_test)\n",
    "            test_regret_epoch_collection.append(regret_epoch_test)\n",
    "            # print out the information\n",
    "            print('The training loss is: ',train_data_loss, ' and the training classification accuracy is: ', train_data_accuracy)\n",
    "            print('The test loss is: ',test_data_loss, ' and the testing classification accuracy is: ', test_data_accuracy)\n",
    "            print('The train regret of the current epoch is: ', regret_epoch_train)\n",
    "            print('The test regret of the current epoch is: ', regret_epoch_test)\n",
    "            print('Recall Info:')\n",
    "            print('On the training data, the recall of Non-trend:', recall_array_train[0], ' Downtrend:', recall_array_train[1], ' Uptrend:', recall_array_train[2])\n",
    "            print('On the testing data, the recall of Non-trend:', recall_array_test[0], ' Downtrend:', recall_array_test[1], ' Uptrend:', recall_array_test[2])\n",
    "            print('Precision Info:')\n",
    "            print('On the training data, the precision of Non-trend:', precision_array_train[0], ' Downtrend:', precision_array_train[1], ' Uptrend:', precision_array_train[2])\n",
    "            print('On the testing data, the precision of Non-trend:', precision_array_test[0], ' Downtrend:', precision_array_test[1], ' Uptrend:', precision_array_test[2])\n",
    "            print('Adjusted Precision:')\n",
    "            print('On the training data, Adjusted Downtrend precision: ', precision_downtrend_adjusted_train, 'Adjusted Uptrend precision: ', precision_uptrend_adjuest_train)\n",
    "            print('On the testing data, Adjusted Downtrend precision: ', precision_downtrend_adjusted_test, 'Adjusted Uptrend precision: ', precision_uptrend_adjuest_test)\n",
    "            print('**************************I\\'m the Divider*****************************')\n",
    "            if(abs((prev_test_loss-currentloss)/currentloss)<=tol):\n",
    "                print('Network stucked to local minimum!')\n",
    "                tf_save_model(sess)\n",
    "                break\n",
    "            # recall of three trends to determine convergence\n",
    "            if (precision_array_train[0]>=0.50)and(recall_array_train[1]>=0.15)and(recall_array_train[2]>=0.15):\n",
    "                save_class_matrix_train = class_matrix_train[:]\n",
    "                save_class_matrix_test = class_matrix_test[:]\n",
    "                tf_save_model(sess)\n",
    "                if regret_epoch_train<=0.65:\n",
    "                    print('Network converged!')\n",
    "                    break\n",
    "            # precision of the non-trend as the convergence critaria\n",
    "            downtrend_satis_condition = abs(precision_array_train[1]-precision_downtrend_adjusted_train)/abs(precision_array_train[1])<1\n",
    "            uptrend_satis_condition = abs(precision_array_train[2]-precision_uptrend_adjuest_train)/abs(precision_array_train[2])<1\n",
    "            if (recall_array_train[0]>=0.50)and((downtrend_satis_condition)or(uptrend_satis_condition)):\n",
    "                save_class_matrix_train = class_matrix_train[:]\n",
    "                save_class_matrix_test = class_matrix_test[:]\n",
    "                tf_save_model(sess)\n",
    "                if regret_epoch_train<=0.65:\n",
    "                    print('Network converged!')\n",
    "                    break\n",
    "            if cEpoch == (nEpochs-1):\n",
    "                print('Network failed to converge!')\n",
    "            prev_test_loss = currentloss\n",
    "    # make the lists as arrays\n",
    "    # train\n",
    "    train_precision_epoch_array = np.array(train_precision_epoch_collection)\n",
    "    train_recall_epoch_array = np.array(train_recall_epoch_collection)\n",
    "    train_regret_epoch_array = np.array(train_regret_epoch_collection)\n",
    "    # put them into one list to save \n",
    "    train_epoch_wise_info = [train_precision_epoch_array, train_recall_epoch_array, train_regret_epoch_array, save_class_matrix_train]\n",
    "    # test\n",
    "    test_precision_epoch_array = np.array(test_precision_epoch_collection)\n",
    "    test_recall_epoch_array = np.array(test_recall_epoch_collection)\n",
    "    test_regret_epoch_array = np.array(test_regret_epoch_collection)\n",
    "    # put them into one list to save\n",
    "    test_epoch_wise_info = [test_precision_epoch_array, test_regret_epoch_array, test_regret_epoch_array, save_class_matrix_test]\n",
    "    \n",
    "    return train_epoch_wise_info, test_epoch_wise_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch_wise_info, test_epoch_wise_info = train_recurrent_neural_network(x_time_series, x_SP_data, x_time_series_prev, x_SP_data_prev, x_current_price)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = '../results/'\n",
    "# make the path if it does not exist yet\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "# save training\n",
    "np.save(save_path+'train_precision_epoch_array.npy',train_epoch_wise_info[0])\n",
    "np.save(save_path+'train_recall_epoch_array.npy',train_epoch_wise_info[1]) \n",
    "np.save(save_path+'train_regret_epoch_array.npy',train_epoch_wise_info[2]) \n",
    "np.save(save_path+'save_class_matrix_train.npy',train_epoch_wise_info[3]) \n",
    "# save the testing\n",
    "np.save(save_path+'test_precision_epoch_array.npy',test_epoch_wise_info[0])\n",
    "np.save(save_path+'test_recall_epoch_array.npy',test_epoch_wise_info[1]) \n",
    "np.save(save_path+'test_regret_epoch_array.npy',test_epoch_wise_info[2]) \n",
    "np.save(save_path+'save_class_matrix_test.npy',test_epoch_wise_info[3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
